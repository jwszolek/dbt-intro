

============================== 2023-02-21 23:24:50.787631 | e12cec22-a3cf-4980-aaba-7db07baddd57 ==============================
[0m23:24:50.787631 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:24:50.788412 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m23:24:50.788541 [debug] [MainThread]: Tracking: tracking
[0m23:24:50.798859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10830d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108341490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108349550>]}
[0m23:24:51.282946 [debug] [MainThread]: Executing "git --help"
[0m23:24:51.289380 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:24:51.289779 [debug] [MainThread]: STDERR: "b''"
[0m23:24:51.293935 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m23:24:51.294428 [debug] [MainThread]: Using databricks connection "debug"
[0m23:24:51.321161 [debug] [MainThread]: On debug: select 1 as id
[0m23:24:51.321412 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:24:52.150262 [debug] [MainThread]: SQL status: OK in 0.83 seconds
[0m23:24:52.152484 [debug] [MainThread]: On debug: Close
[0m23:24:52.415018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076e4c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2b4580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2b4880>]}
[0m23:24:52.416033 [debug] [MainThread]: Flushing usage events
[0m23:24:52.980704 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-21 23:30:43.726227 | 800e7e57-d0f3-4636-af9e-26ba3d59f31f ==============================
[0m23:30:43.726227 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:30:43.727865 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:30:43.728049 [debug] [MainThread]: Tracking: tracking
[0m23:30:43.739244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a71c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a7640>]}
[0m23:30:43.784496 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:30:43.784777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268334f0>]}
[0m23:30:44.254043 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:30:44.260579 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:30:44.262538 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:30:44.300025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b2b970>]}
[0m23:30:44.303474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b43100>]}
[0m23:30:44.303654 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:30:44.303880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b92e50>]}
[0m23:30:44.304403 [info ] [MainThread]: 
[0m23:30:44.305256 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:30:44.305734 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:30:44.311147 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:30:44.311369 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:30:44.311501 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:30:44.929648 [debug] [ThreadPool]: SQL status: OK in 0.62 seconds
[0m23:30:44.942414 [debug] [ThreadPool]: On list_schemas: Close
[0m23:30:45.103771 [debug] [ThreadPool]: Acquiring new databricks connection 'create__dbt_silver'
[0m23:30:45.105082 [debug] [ThreadPool]: Acquiring new databricks connection 'create__dbt_silver'
[0m23:30:45.105792 [debug] [ThreadPool]: Creating schema "ReferenceKeyMsg(database=None, schema='dbt_silver', identifier=None)"
[0m23:30:45.116022 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:45.116449 [debug] [ThreadPool]: Using databricks connection "create__dbt_silver"
[0m23:30:45.116821 [debug] [ThreadPool]: On create__dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "create__dbt_silver"} */
create schema if not exists `dbt_silver`
  
[0m23:30:45.117177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:30:46.028191 [debug] [ThreadPool]: SQL status: OK in 0.91 seconds
[0m23:30:46.030964 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m23:30:46.031633 [debug] [ThreadPool]: On create__dbt_silver: ROLLBACK
[0m23:30:46.032131 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:30:46.032590 [debug] [ThreadPool]: On create__dbt_silver: Close
[0m23:30:46.190971 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_dbt_silver'
[0m23:30:46.216225 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:46.216549 [debug] [ThreadPool]: Using databricks connection "list_None_dbt_silver"
[0m23:30:46.216751 [debug] [ThreadPool]: On list_None_dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_dbt_silver"} */
show tables in `dbt_silver`
  
[0m23:30:46.217164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:30:46.809179 [debug] [ThreadPool]: SQL status: OK in 0.59 seconds
[0m23:30:46.815341 [debug] [ThreadPool]: On list_None_dbt_silver: ROLLBACK
[0m23:30:46.815854 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:30:46.816237 [debug] [ThreadPool]: On list_None_dbt_silver: Close
[0m23:30:46.964481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b63b80>]}
[0m23:30:46.965500 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:46.965962 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:30:46.967017 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:30:46.967677 [info ] [MainThread]: 
[0m23:30:46.973086 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:30:46.973673 [info ] [Thread-1  ]: 1 of 1 START sql table model dbt_silver.silver_cities .......................... [RUN]
[0m23:30:46.974763 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:30:46.975093 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:30:46.978257 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:30:46.979224 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:30:46.975349 => 2023-02-21 23:30:46.979159
[0m23:30:46.979551 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:30:47.022599 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:30:47.023109 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:47.023275 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:30:47.023439 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from bronze_cities
order by id 
limit 5
  
[0m23:30:47.023582 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:30:47.755025 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from bronze_cities
order by id 
limit 5
  
[0m23:30:47.755970 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:47.756405 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:30:47.756847 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2?\xc4\xe1\x16\xf9\x8e\xe3\x12?\xa6\xf4K\xf7'
[0m23:30:47.757792 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:30:46.979795 => 2023-02-21 23:30:47.757610
[0m23:30:47.758357 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:30:47.758857 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:30:47.759306 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:30:47.954263 [debug] [Thread-1  ]: Runtime Error in model silver_cities (models/silver_cities.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:47.954773 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126c4efa0>]}
[0m23:30:47.955218 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dbt_silver.silver_cities ................. [[31mERROR[0m in 0.98s]
[0m23:30:47.956434 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:30:47.957403 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:30:47.957643 [debug] [MainThread]: On master: ROLLBACK
[0m23:30:47.957875 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:30:48.148222 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:30:48.149597 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:48.150145 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:30:48.150720 [debug] [MainThread]: On master: ROLLBACK
[0m23:30:48.151187 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:30:48.151630 [debug] [MainThread]: On master: Close
[0m23:30:48.318373 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:30:48.319325 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:30:48.321905 [info ] [MainThread]: 
[0m23:30:48.322659 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.02 seconds (4.02s).
[0m23:30:48.323540 [debug] [MainThread]: Command end result
[0m23:30:48.337862 [info ] [MainThread]: 
[0m23:30:48.338483 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:30:48.338915 [info ] [MainThread]: 
[0m23:30:48.339340 [error] [MainThread]: [33mRuntime Error in model silver_cities (models/silver_cities.sql)[0m
[0m23:30:48.339753 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:30:48.340171 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:30:48.340571 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:48.340975 [info ] [MainThread]: 
[0m23:30:48.341383 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:30:48.342053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b13190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b5bd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b92e50>]}
[0m23:30:48.342352 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:32:48.083317 | 1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea ==============================
[0m23:32:48.083317 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:32:48.084938 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:32:48.085121 [debug] [MainThread]: Tracking: tracking
[0m23:32:48.103712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152ace1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152ace670>]}
[0m23:32:48.150856 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m23:32:48.151178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a75a60>]}
[0m23:32:48.611278 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:32:48.617812 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:32:48.619727 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:32:48.656771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d7d0d0>]}
[0m23:32:48.660541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152b3d580>]}
[0m23:32:48.660724 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:32:48.660932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d9f5e0>]}
[0m23:32:48.661449 [info ] [MainThread]: 
[0m23:32:48.662287 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:32:48.662781 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:32:48.668149 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:32:48.668367 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:32:48.668509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:32:49.435798 [debug] [ThreadPool]: SQL status: OK in 0.77 seconds
[0m23:32:49.451627 [debug] [ThreadPool]: On list_schemas: Close
[0m23:32:49.641563 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_dbt_silver'
[0m23:32:49.655737 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:49.656121 [debug] [ThreadPool]: Using databricks connection "list_None_dbt_silver"
[0m23:32:49.656457 [debug] [ThreadPool]: On list_None_dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_dbt_silver"} */
show tables in `dbt_silver`
  
[0m23:32:49.656752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:32:50.329764 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:32:50.335021 [debug] [ThreadPool]: On list_None_dbt_silver: ROLLBACK
[0m23:32:50.335521 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:32:50.335890 [debug] [ThreadPool]: On list_None_dbt_silver: Close
[0m23:32:50.484295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66ee0>]}
[0m23:32:50.485538 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:50.486021 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:32:50.487102 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:32:50.487809 [info ] [MainThread]: 
[0m23:32:50.496112 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:32:50.496794 [info ] [Thread-1  ]: 1 of 1 START sql table model dbt_silver.silver_cities .......................... [RUN]
[0m23:32:50.497695 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:32:50.497987 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:32:50.502179 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:32:50.502875 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:32:50.498287 => 2023-02-21 23:32:50.502817
[0m23:32:50.503153 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:32:50.549932 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:32:50.550302 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:50.550479 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:32:50.550648 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 5
  
[0m23:32:50.550792 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:33:03.864970 [debug] [Thread-1  ]: SQL status: OK in 13.31 seconds
[0m23:33:04.112748 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:32:50.503425 => 2023-02-21 23:33:04.112673
[0m23:33:04.113209 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:33:04.113482 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.113712 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:33:04.288546 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152f60820>]}
[0m23:33:04.290042 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dbt_silver.silver_cities ..................... [[32mOK[0m in 13.79s]
[0m23:33:04.292859 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:33:04.295241 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:33:04.295765 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:04.296256 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:33:04.486833 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.487999 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:04.488634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:33:04.489269 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:04.489706 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.490130 [debug] [MainThread]: On master: Close
[0m23:33:04.638594 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:33:04.639688 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:33:04.642460 [info ] [MainThread]: 
[0m23:33:04.643415 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 15.98 seconds (15.98s).
[0m23:33:04.644520 [debug] [MainThread]: Command end result
[0m23:33:04.657309 [info ] [MainThread]: 
[0m23:33:04.657847 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:33:04.658362 [info ] [MainThread]: 
[0m23:33:04.658798 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:33:04.659315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152db1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66520>]}
[0m23:33:04.659709 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:35:21.902133 | ded5eafc-3ede-4f61-8b58-7664fb8cc89b ==============================
[0m23:35:21.902133 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:35:21.903726 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:35:21.904045 [debug] [MainThread]: Tracking: tracking
[0m23:35:21.916562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f56160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f56f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f565e0>]}
[0m23:35:21.958141 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m23:35:21.958464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135dae430>]}
[0m23:35:22.429017 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:35:22.435478 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:35:22.437305 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:35:22.474769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a3b0d0>]}
[0m23:35:22.478327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1368494f0>]}
[0m23:35:22.478518 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:35:22.478734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a1e550>]}
[0m23:35:22.479259 [info ] [MainThread]: 
[0m23:35:22.480117 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:35:22.480569 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:35:22.485833 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:35:22.486062 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:35:22.486192 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:35:23.222967 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
[0m23:35:23.237583 [debug] [ThreadPool]: On list_schemas: Close
[0m23:35:23.440570 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:35:23.457973 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:23.458345 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:35:23.458627 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:35:23.458888 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:35:24.215954 [debug] [ThreadPool]: SQL status: OK in 0.76 seconds
[0m23:35:24.231466 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:35:24.231968 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:35:24.637274 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:35:24.642211 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:35:24.642777 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:35:24.643193 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:35:24.800287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a2ceb0>]}
[0m23:35:24.801093 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:24.801546 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:35:24.802496 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:35:24.803196 [info ] [MainThread]: 
[0m23:35:24.809006 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:35:24.809642 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities ............................. [RUN]
[0m23:35:24.810731 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:35:24.811178 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:35:24.813431 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:35:24.814026 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:35:24.811452 => 2023-02-21 23:35:24.813977
[0m23:35:24.814248 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:35:24.858626 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:35:24.859135 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:24.859326 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:35:24.859515 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `airbyte`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 5
  
[0m23:35:24.859670 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:35:28.231376 [debug] [Thread-1  ]: SQL status: OK in 3.37 seconds
[0m23:35:28.260117 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:35:24.814397 => 2023-02-21 23:35:28.260049
[0m23:35:28.260527 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:35:28.260795 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.261027 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:35:28.431563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40370>]}
[0m23:35:28.433417 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities ........................ [[32mOK[0m in 3.62s]
[0m23:35:28.437494 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:35:28.440166 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:35:28.440821 [debug] [MainThread]: On master: ROLLBACK
[0m23:35:28.441345 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:35:28.626491 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.627827 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:28.628843 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:35:28.629235 [debug] [MainThread]: On master: ROLLBACK
[0m23:35:28.629566 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.629892 [debug] [MainThread]: On master: Close
[0m23:35:28.795444 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:35:28.796135 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:35:28.798219 [info ] [MainThread]: 
[0m23:35:28.798922 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.32 seconds (6.32s).
[0m23:35:28.799786 [debug] [MainThread]: Command end result
[0m23:35:28.810162 [info ] [MainThread]: 
[0m23:35:28.810859 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:35:28.811408 [info ] [MainThread]: 
[0m23:35:28.811911 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:35:28.812493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1368e9af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40460>]}
[0m23:35:28.812928 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:42:34.246742 | da583ba4-159b-4056-bc91-b3e7001f959f ==============================
[0m23:42:34.246742 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:42:34.248259 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:42:34.248433 [debug] [MainThread]: Tracking: tracking
[0m23:42:34.266574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216880>]}
[0m23:42:34.324465 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m23:42:34.324711 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/gold_users_cities_join.sql
[0m23:42:34.333128 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:42:34.345119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12237a2e0>]}
[0m23:42:34.348777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103196f70>]}
[0m23:42:34.348983 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:42:34.349196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103921250>]}
[0m23:42:34.349711 [info ] [MainThread]: 
[0m23:42:34.350545 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:42:34.351011 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:42:34.356410 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:42:34.356626 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:42:34.356755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:42:35.243623 [debug] [ThreadPool]: SQL status: OK in 0.89 seconds
[0m23:42:35.260039 [debug] [ThreadPool]: On list_schemas: Close
[0m23:42:35.434409 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:42:35.448800 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:35.449216 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:42:35.449530 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:42:35.449825 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:42:36.134514 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m23:42:36.150392 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:42:36.150819 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:42:36.567541 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:42:36.573556 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:42:36.574201 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:42:36.574634 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:42:36.737489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122418df0>]}
[0m23:42:36.738878 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:36.739473 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:42:36.740666 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:42:36.741385 [info ] [MainThread]: 
[0m23:42:36.748622 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:42:36.749105 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:42:36.750002 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:42:36.750392 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:42:36.754197 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.755025 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:42:36.750692 => 2023-02-21 23:42:36.754953
[0m23:42:36.755406 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:42:36.781473 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.781963 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:36.782167 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.782369 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:42:36.782540 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:42:37.534185 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:42:37.535426 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:37.535800 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:42:37.536215 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2Ak\xee\x1c\xdf\xa6\xb3\xb3\x83Q\xb3\xa8\x86'
[0m23:42:37.537030 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:42:36.755661 => 2023-02-21 23:42:37.536886
[0m23:42:37.537491 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:42:37.537860 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.538213 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:42:37.733370 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:37.733793 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12248b790>]}
[0m23:42:37.734179 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.98s]
[0m23:42:37.735424 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:42:37.736420 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:42:37.736660 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:37.736848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:42:37.909773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.910748 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:37.911190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:42:37.911668 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:37.912111 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.912547 [debug] [MainThread]: On master: Close
[0m23:42:38.061349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:42:38.062482 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:42:38.065239 [info ] [MainThread]: 
[0m23:42:38.066004 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.72 seconds (3.72s).
[0m23:42:38.066881 [debug] [MainThread]: Command end result
[0m23:42:38.080015 [info ] [MainThread]: 
[0m23:42:38.080681 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:42:38.081127 [info ] [MainThread]: 
[0m23:42:38.081557 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:42:38.081988 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:42:38.082383 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:42:38.082653 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:38.082930 [info ] [MainThread]: 
[0m23:42:38.083267 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:42:38.083769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12235fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1031962b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122480a30>]}
[0m23:42:38.084143 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:43:22.731311 | a420e9a4-e1ab-43e2-b68a-5db235ea6f56 ==============================
[0m23:43:22.731311 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:43:22.733040 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:43:22.733342 [debug] [MainThread]: Tracking: tracking
[0m23:43:22.753854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acf160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acf5e0>]}
[0m23:43:22.807343 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:43:22.807652 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:43:22.816069 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:43:22.827819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152c321f0>]}
[0m23:43:22.831614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a56160>]}
[0m23:43:22.831795 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:43:22.832007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152c12fd0>]}
[0m23:43:22.832532 [info ] [MainThread]: 
[0m23:43:22.833371 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:43:22.833864 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:43:22.839528 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:43:22.839792 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:43:22.839942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:43:23.417086 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m23:43:23.433122 [debug] [ThreadPool]: On list_schemas: Close
[0m23:43:23.589388 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:43:23.606843 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:23.607221 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:43:23.607529 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:43:23.607789 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:43:24.282955 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:43:24.298979 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:43:24.299381 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:43:24.703574 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:43:24.708320 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:43:24.708846 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:43:24.709275 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:43:24.862270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152cdb4f0>]}
[0m23:43:24.863060 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:24.863449 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:43:24.864313 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:43:24.864844 [info ] [MainThread]: 
[0m23:43:24.878034 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:43:24.878459 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:43:24.879097 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:43:24.879308 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:43:24.881095 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.881523 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:43:24.879450 => 2023-02-21 23:43:24.881482
[0m23:43:24.881686 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:43:24.899962 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.900451 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:24.900626 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.900774 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    -- 

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:43:24.900909 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:43:25.714161 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    -- 

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:43:25.715066 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:25.715396 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:43:25.715744 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\x88\x99\x10\xc8\xb5W\xdd\x90\x04\xc9!\x08'
[0m23:43:25.716510 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:43:24.881807 => 2023-02-21 23:43:25.716373
[0m23:43:25.717044 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:43:25.717407 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:43:25.717724 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:43:25.907425 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:25.907856 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152e4fbe0>]}
[0m23:43:25.908270 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 1.03s]
[0m23:43:25.909466 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:43:25.910387 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:43:25.910625 [debug] [MainThread]: On master: ROLLBACK
[0m23:43:25.910820 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:43:26.098017 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:43:26.099509 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:26.100364 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:43:26.100781 [debug] [MainThread]: On master: ROLLBACK
[0m23:43:26.101115 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:43:26.101440 [debug] [MainThread]: On master: Close
[0m23:43:26.260087 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:43:26.261164 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:43:26.263525 [info ] [MainThread]: 
[0m23:43:26.264343 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.43 seconds (3.43s).
[0m23:43:26.265398 [debug] [MainThread]: Command end result
[0m23:43:26.278560 [info ] [MainThread]: 
[0m23:43:26.279151 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:43:26.279583 [info ] [MainThread]: 
[0m23:43:26.279925 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:43:26.280206 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:43:26.280542 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:43:26.280949 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:26.281369 [info ] [MainThread]: 
[0m23:43:26.281773 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:43:26.282276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a56040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108827d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a74ee0>]}
[0m23:43:26.282671 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:44:40.118576 | 11ae0a59-e64f-41a3-8f25-ff5e7978cc2c ==============================
[0m23:44:40.118576 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:44:40.123226 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:44:40.123485 [debug] [MainThread]: Tracking: tracking
[0m23:44:40.142252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1269561f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126956f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126956670>]}
[0m23:44:40.198776 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:44:40.199054 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:44:40.206376 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:44:40.218046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bbc2b0>]}
[0m23:44:40.221671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105834280>]}
[0m23:44:40.221859 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:44:40.222067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10644fe50>]}
[0m23:44:40.222594 [info ] [MainThread]: 
[0m23:44:40.223424 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:44:40.223904 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:44:40.229241 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:44:40.229465 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:44:40.229596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:40.881592 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:44:40.899177 [debug] [ThreadPool]: On list_schemas: Close
[0m23:44:41.124481 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:44:41.135352 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:41.135675 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:44:41.135942 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:44:41.136192 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:41.805808 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:44:41.823055 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:44:41.823457 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:44:42.223681 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:44:42.230505 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:44:42.231097 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:44:42.231520 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:44:42.383239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126c63790>]}
[0m23:44:42.384443 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:42.384980 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:44:42.386149 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:44:42.386851 [info ] [MainThread]: 
[0m23:44:42.394612 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:44:42.395247 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:44:42.396366 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:44:42.396764 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:44:42.399812 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.401178 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:44:42.397119 => 2023-02-21 23:44:42.401116
[0m23:44:42.401507 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:44:42.427142 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.429385 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:42.429596 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.429806 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:44:42.429984 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:42.914848 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:44:42.915734 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:44:42.916881 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:44:42.919614 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\xb6\xcd\x15`\x9c\xa6\xd6\xe1\x025\x82\x8b'
[0m23:44:42.921044 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:44:42.401750 => 2023-02-21 23:44:42.920895
[0m23:44:42.921597 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:44:42.922158 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:44:42.922626 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:44:43.122741 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  create or replace view `airbyte`.`gold_users_cities_join`
    
    
    as
      
  
  DROP TABLE IF EXISTS snowflake_silver_users;
  ^^^
  CREATE TABLE snowflake_silver_users
  USING snowflake
  OPTIONS (
      dbtable 'silver_users',
      sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
      sfUser 'jwszolek',
      sfPassword '.8kki-cqP@f2W@7zYY46',
      sfDatabase 'jwszol',
      sfSchema 'public',
      sfWarehouse 'COMPUTE_WH'
  );
  
  select * from snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.ID == dbx.user_id
  
[0m23:44:43.123228 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126cd0fa0>]}
[0m23:44:43.123677 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.73s]
[0m23:44:43.124913 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:44:43.125800 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:44:43.126044 [debug] [MainThread]: On master: ROLLBACK
[0m23:44:43.126243 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:44:43.302826 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:44:43.304219 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:43.305083 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:44:43.305939 [debug] [MainThread]: On master: ROLLBACK
[0m23:44:43.306743 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:44:43.307557 [debug] [MainThread]: On master: Close
[0m23:44:43.465575 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:44:43.466517 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:44:43.469374 [info ] [MainThread]: 
[0m23:44:43.470352 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.25 seconds (3.25s).
[0m23:44:43.471415 [debug] [MainThread]: Command end result
[0m23:44:43.484600 [info ] [MainThread]: 
[0m23:44:43.485187 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:44:43.485553 [info ] [MainThread]: 
[0m23:44:43.485939 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:44:43.486352 [error] [MainThread]:   
[0m23:44:43.486748 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)
[0m23:44:43.487138 [error] [MainThread]:   
[0m23:44:43.487521 [error] [MainThread]:   == SQL ==
[0m23:44:43.487902 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m23:44:43.488281 [error] [MainThread]:   create or replace view `airbyte`.`gold_users_cities_join`
[0m23:44:43.488659 [error] [MainThread]:     
[0m23:44:43.489038 [error] [MainThread]:     
[0m23:44:43.489423 [error] [MainThread]:     as
[0m23:44:43.489805 [error] [MainThread]:       
[0m23:44:43.490189 [error] [MainThread]:   
[0m23:44:43.490614 [error] [MainThread]:   DROP TABLE IF EXISTS snowflake_silver_users;
[0m23:44:43.491037 [error] [MainThread]:   ^^^
[0m23:44:43.491429 [error] [MainThread]:   CREATE TABLE snowflake_silver_users
[0m23:44:43.491798 [error] [MainThread]:   USING snowflake
[0m23:44:43.492150 [error] [MainThread]:   OPTIONS (
[0m23:44:43.492479 [error] [MainThread]:       dbtable 'silver_users',
[0m23:44:43.492816 [error] [MainThread]:       sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m23:44:43.493167 [error] [MainThread]:       sfUser 'jwszolek',
[0m23:44:43.493502 [error] [MainThread]:       sfPassword '.8kki-cqP@f2W@7zYY46',
[0m23:44:43.493843 [error] [MainThread]:       sfDatabase 'jwszol',
[0m23:44:43.494179 [error] [MainThread]:       sfSchema 'public',
[0m23:44:43.494503 [error] [MainThread]:       sfWarehouse 'COMPUTE_WH'
[0m23:44:43.494827 [error] [MainThread]:   );
[0m23:44:43.495153 [error] [MainThread]:   
[0m23:44:43.495480 [error] [MainThread]:   select * from snowflake_silver_users snow
[0m23:44:43.495853 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.ID == dbx.user_id
[0m23:44:43.496206 [error] [MainThread]:   
[0m23:44:43.496554 [info ] [MainThread]: 
[0m23:44:43.496903 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:44:43.497316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1267addf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10590f610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126cc3a90>]}
[0m23:44:43.497619 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:45:18.163813 | 0c6a835f-ddca-4729-a184-8156f2714dbc ==============================
[0m23:45:18.163813 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:45:18.165197 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:45:18.165445 [debug] [MainThread]: Tracking: tracking
[0m23:45:18.183218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a4531f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a453f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a453670>]}
[0m23:45:18.257330 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:45:18.257691 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:45:18.265932 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:45:18.278695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a5b8790>]}
[0m23:45:18.282595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12128fd90>]}
[0m23:45:18.282818 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:45:18.283033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1220bfe50>]}
[0m23:45:18.283607 [info ] [MainThread]: 
[0m23:45:18.284481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:18.285020 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:45:18.290844 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:45:18.291089 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:45:18.291240 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:18.893376 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m23:45:18.917055 [debug] [ThreadPool]: On list_schemas: Close
[0m23:45:19.064114 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:45:19.076196 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:19.076538 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:45:19.076814 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:45:19.077080 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:19.788935 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m23:45:19.806553 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:45:19.807050 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:45:20.205985 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:45:20.213104 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:45:20.213749 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:45:20.214182 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:45:20.412306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a65f790>]}
[0m23:45:20.413364 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:20.413880 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:45:20.414967 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:45:20.415717 [info ] [MainThread]: 
[0m23:45:20.423525 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:45:20.424184 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:45:20.425060 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:45:20.425321 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:45:20.427760 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.431105 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:45:20.425505 => 2023-02-21 23:45:20.431055
[0m23:45:20.431334 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:45:20.452457 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.453205 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:20.453463 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.453728 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:45:20.453942 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:45:20.926008 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:45:20.928482 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:45:20.929050 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:45:20.929445 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\xcdw\x1f\xf7\x83Y\x04\xf1f^\xe3\x84'
[0m23:45:20.930293 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:45:20.431449 => 2023-02-21 23:45:20.930148
[0m23:45:20.930765 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:45:20.931138 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:45:20.931468 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:45:21.129951 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  create or replace view `airbyte`.`gold_users_cities_join`
    
    
    as
      
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  CREATE TABLE snowflake_silver_users
  ^^^
  USING snowflake
  OPTIONS (
      dbtable 'silver_users',
      sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
      sfUser 'jwszolek',
      sfPassword '.8kki-cqP@f2W@7zYY46',
      sfDatabase 'jwszol',
      sfSchema 'public',
      sfWarehouse 'COMPUTE_WH'
  );
  
  select * from snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.ID == dbx.user_id
  
[0m23:45:21.130362 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a6cdfa0>]}
[0m23:45:21.130687 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.71s]
[0m23:45:21.131655 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:45:21.132437 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:21.132626 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:21.132779 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:45:21.311724 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:45:21.313401 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:21.314174 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:45:21.314631 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:21.315028 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:45:21.315422 [debug] [MainThread]: On master: Close
[0m23:45:21.566590 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:45:21.568108 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:45:21.570654 [info ] [MainThread]: 
[0m23:45:21.571221 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.29 seconds (3.29s).
[0m23:45:21.571839 [debug] [MainThread]: Command end result
[0m23:45:21.580925 [info ] [MainThread]: 
[0m23:45:21.581407 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:45:21.581696 [info ] [MainThread]: 
[0m23:45:21.581977 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:45:21.582286 [error] [MainThread]:   
[0m23:45:21.582629 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)
[0m23:45:21.582963 [error] [MainThread]:   
[0m23:45:21.583304 [error] [MainThread]:   == SQL ==
[0m23:45:21.583644 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m23:45:21.583981 [error] [MainThread]:   create or replace view `airbyte`.`gold_users_cities_join`
[0m23:45:21.584322 [error] [MainThread]:     
[0m23:45:21.584660 [error] [MainThread]:     
[0m23:45:21.585001 [error] [MainThread]:     as
[0m23:45:21.585329 [error] [MainThread]:       
[0m23:45:21.585662 [error] [MainThread]:   
[0m23:45:21.586005 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m23:45:21.586328 [error] [MainThread]:   CREATE TABLE snowflake_silver_users
[0m23:45:21.586668 [error] [MainThread]:   ^^^
[0m23:45:21.586989 [error] [MainThread]:   USING snowflake
[0m23:45:21.587311 [error] [MainThread]:   OPTIONS (
[0m23:45:21.587592 [error] [MainThread]:       dbtable 'silver_users',
[0m23:45:21.587871 [error] [MainThread]:       sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m23:45:21.588149 [error] [MainThread]:       sfUser 'jwszolek',
[0m23:45:21.588427 [error] [MainThread]:       sfPassword '.8kki-cqP@f2W@7zYY46',
[0m23:45:21.588704 [error] [MainThread]:       sfDatabase 'jwszol',
[0m23:45:21.588980 [error] [MainThread]:       sfSchema 'public',
[0m23:45:21.589257 [error] [MainThread]:       sfWarehouse 'COMPUTE_WH'
[0m23:45:21.589534 [error] [MainThread]:   );
[0m23:45:21.589812 [error] [MainThread]:   
[0m23:45:21.590097 [error] [MainThread]:   select * from snowflake_silver_users snow
[0m23:45:21.590391 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.ID == dbx.user_id
[0m23:45:21.590684 [error] [MainThread]:   
[0m23:45:21.590981 [info ] [MainThread]: 
[0m23:45:21.591274 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:45:21.591641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a3d9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c28e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a6c0a90>]}
[0m23:45:21.591939 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:53:19.578639 | 75433c39-43de-4261-a19a-cb9196637fbb ==============================
[0m23:53:19.578639 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:53:19.580111 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:53:19.580346 [debug] [MainThread]: Tracking: tracking
[0m23:53:19.591039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a931f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a93f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a93670>]}
[0m23:53:19.649667 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:53:19.649949 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:53:19.657303 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:53:19.669092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bf9a30>]}
[0m23:53:19.672993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065c1280>]}
[0m23:53:19.673185 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:53:19.673406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072bfe50>]}
[0m23:53:19.673966 [info ] [MainThread]: 
[0m23:53:19.674800 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:53:19.675275 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:53:19.681112 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:53:19.681379 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:53:19.681526 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:20.281052 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m23:53:20.296373 [debug] [ThreadPool]: On list_schemas: Close
[0m23:53:20.466786 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:53:20.485705 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:20.486192 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:53:20.486513 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:53:20.486811 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:53:21.133830 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:53:21.151304 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:53:21.151715 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:53:21.562260 [debug] [ThreadPool]: SQL status: OK in 0.41 seconds
[0m23:53:21.568448 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:53:21.569084 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:53:21.569539 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:53:21.817006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126da3790>]}
[0m23:53:21.818372 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:21.818897 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:53:21.820007 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:53:21.822600 [info ] [MainThread]: 
[0m23:53:21.830219 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:53:21.830962 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:53:21.832125 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:53:21.832538 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:53:21.836003 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.837051 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:53:21.832848 => 2023-02-21 23:53:21.836976
[0m23:53:21.837423 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:53:21.861030 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.861496 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:21.861701 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.861914 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users
-- join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:53:21.862084 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:53:23.197341 [debug] [Thread-1  ]: SQL status: OK in 1.33 seconds
[0m23:53:23.217883 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:53:21.837680 => 2023-02-21 23:53:23.217804
[0m23:53:23.218363 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:53:23.218676 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.218951 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:53:23.366315 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f22370>]}
[0m23:53:23.367815 [info ] [Thread-1  ]: 1 of 1 OK created sql view model airbyte.gold_users_cities_join ................ [[32mOK[0m in 1.53s]
[0m23:53:23.371303 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:53:23.375461 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:53:23.376335 [debug] [MainThread]: On master: ROLLBACK
[0m23:53:23.376777 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:53:23.561286 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.562503 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:23.562978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:53:23.563485 [debug] [MainThread]: On master: ROLLBACK
[0m23:53:23.563921 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.564366 [debug] [MainThread]: On master: Close
[0m23:53:23.721194 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:53:23.722191 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:53:23.724996 [info ] [MainThread]: 
[0m23:53:23.725891 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.05 seconds (4.05s).
[0m23:53:23.726995 [debug] [MainThread]: Command end result
[0m23:53:23.738780 [info ] [MainThread]: 
[0m23:53:23.739382 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:53:23.739835 [info ] [MainThread]: 
[0m23:53:23.740264 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:53:23.740810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f30f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f02a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bd9100>]}
[0m23:53:23.741192 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:55:07.911025 | 9afe1ebd-d6f4-4b51-98f4-6e5a81c06801 ==============================
[0m23:55:07.911025 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:55:07.912713 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:55:07.912920 [debug] [MainThread]: Tracking: tracking
[0m23:55:07.924453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5941f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d594f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d594670>]}
[0m23:55:07.988547 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:55:07.988840 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:55:07.996110 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:55:08.007841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6fb430>]}
[0m23:55:08.011627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106beed90>]}
[0m23:55:08.011812 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:55:08.012031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077fce50>]}
[0m23:55:08.012547 [info ] [MainThread]: 
[0m23:55:08.013452 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:55:08.014000 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:55:08.019807 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:55:08.020036 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:55:08.020163 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:55:08.646979 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m23:55:08.665249 [debug] [ThreadPool]: On list_schemas: Close
[0m23:55:08.846610 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:55:08.867437 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:08.867826 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:55:08.868104 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:55:08.868371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:55:09.480896 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:55:09.493383 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:55:09.493654 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:55:10.008072 [debug] [ThreadPool]: SQL status: OK in 0.51 seconds
[0m23:55:10.015342 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:55:10.016158 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:55:10.016615 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:55:10.171191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923460>]}
[0m23:55:10.172634 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:10.173216 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:55:10.174385 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:55:10.175125 [info ] [MainThread]: 
[0m23:55:10.183019 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:55:10.183723 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:55:10.184955 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:55:10.185289 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:55:10.189229 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:55:10.190242 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:55:10.185600 => 2023-02-21 23:55:10.190190
[0m23:55:10.190473 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:55:10.205104 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:10.205381 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:10.205613 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:55:10.205819 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:55:11.241516 [debug] [Thread-1  ]: SQL status: OK in 1.04 seconds
[0m23:55:11.259615 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:11.260014 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
drop view if exists `airbyte`.`gold_users_cities_join`
[0m23:55:12.635675 [debug] [Thread-1  ]: SQL status: OK in 1.38 seconds
[0m23:55:12.678321 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:55:12.678885 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:12.679122 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users
-- join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:55:19.438266 [debug] [Thread-1  ]: SQL status: OK in 6.76 seconds
[0m23:55:19.733187 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:55:10.190638 => 2023-02-21 23:55:19.733115
[0m23:55:19.733632 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:55:19.733898 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:55:19.734130 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:55:19.882971 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923370>]}
[0m23:55:19.884306 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 9.70s]
[0m23:55:19.887406 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:55:19.890492 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:55:19.891174 [debug] [MainThread]: On master: ROLLBACK
[0m23:55:19.891636 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:55:20.084701 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:55:20.085999 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:20.086478 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:55:20.086982 [debug] [MainThread]: On master: ROLLBACK
[0m23:55:20.087406 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:55:20.087800 [debug] [MainThread]: On master: Close
[0m23:55:20.238311 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:55:20.239338 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:55:20.242153 [info ] [MainThread]: 
[0m23:55:20.242982 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.23 seconds (12.23s).
[0m23:55:20.243986 [debug] [MainThread]: Command end result
[0m23:55:20.260727 [info ] [MainThread]: 
[0m23:55:20.261345 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:55:20.261786 [info ] [MainThread]: 
[0m23:55:20.262231 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:55:20.262748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d903c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d930f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923460>]}
[0m23:55:20.263129 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:57:11.132130 | f8e7d765-99dc-41a1-8436-9348a88c44a9 ==============================
[0m23:57:11.132130 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:57:11.133703 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:57:11.133908 [debug] [MainThread]: Tracking: tracking
[0m23:57:11.148929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6610>]}
[0m23:57:11.207153 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:57:11.207438 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:57:11.216073 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:57:11.228002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a23f5b0>]}
[0m23:57:11.231552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129f636a0>]}
[0m23:57:11.231749 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:57:11.231976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103579b50>]}
[0m23:57:11.232496 [info ] [MainThread]: 
[0m23:57:11.233350 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:11.233794 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:57:11.239498 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:57:11.239732 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:57:11.239865 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:11.902815 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
[0m23:57:11.917152 [debug] [ThreadPool]: On list_schemas: Close
[0m23:57:12.094127 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:57:12.109145 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:12.109555 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:12.109878 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:57:12.110172 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:12.817945 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m23:57:12.835301 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:12.835755 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:57:13.254265 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:57:13.260839 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:57:13.261457 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:57:13.261834 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:57:13.412614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a2e20d0>]}
[0m23:57:13.414035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:13.414496 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:13.415543 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:57:13.416085 [info ] [MainThread]: 
[0m23:57:13.424605 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:57:13.425245 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:57:13.426356 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:57:13.426749 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:57:13.430357 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:13.431350 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:57:13.427054 => 2023-02-21 23:57:13.431281
[0m23:57:13.431695 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:57:13.446167 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:13.446444 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:13.446671 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:57:13.446865 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:57:14.146539 [debug] [Thread-1  ]: SQL status: OK in 0.7 seconds
[0m23:57:14.192325 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:14.192832 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:14.193043 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:57:14.897205 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:57:14.898668 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:14.899128 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:57:14.899554 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2Cv\xc6\x13\xb3\x9c\xff\xc2\x88\xfe\x10\x92\xec'
[0m23:57:14.900468 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:57:13.431946 => 2023-02-21 23:57:14.900313
[0m23:57:14.901002 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:57:14.901410 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:57:14.901806 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:57:15.097131 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:15.097562 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a363370>]}
[0m23:57:15.097941 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.67s]
[0m23:57:15.099508 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:57:15.100787 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:15.101054 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:15.101259 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:15.276962 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:15.278646 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:15.279214 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:15.279726 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:15.280172 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:15.280603 [debug] [MainThread]: On master: Close
[0m23:57:15.426519 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:15.427037 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:57:15.429328 [info ] [MainThread]: 
[0m23:57:15.430338 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.20 seconds (4.20s).
[0m23:57:15.431638 [debug] [MainThread]: Command end result
[0m23:57:15.442839 [info ] [MainThread]: 
[0m23:57:15.443611 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:57:15.444169 [info ] [MainThread]: 
[0m23:57:15.444550 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:57:15.444831 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:15.445079 [info ] [MainThread]: 
[0m23:57:15.445431 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:57:15.445766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129f636a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a342bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a21ec40>]}
[0m23:57:15.446008 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:57:50.685111 | 8a978594-9b15-49c3-9678-9efe2fcd5663 ==============================
[0m23:57:50.685111 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:57:50.686813 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:57:50.687015 [debug] [MainThread]: Tracking: tracking
[0m23:57:50.698879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12610>]}
[0m23:57:50.754896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:57:50.755176 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:57:50.763844 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:57:50.775903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cd7e7f0>]}
[0m23:57:50.779615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca9f6a0>]}
[0m23:57:50.779802 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:57:50.780017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039f1b50>]}
[0m23:57:50.780549 [info ] [MainThread]: 
[0m23:57:50.781396 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:50.781876 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:57:50.787325 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:57:50.787570 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:57:50.787714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:51.441480 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:57:51.456170 [debug] [ThreadPool]: On list_schemas: Close
[0m23:57:51.686797 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:57:51.701007 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:51.701393 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:51.701669 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:57:51.701922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:52.315924 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:57:52.330480 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:52.330837 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:57:52.754348 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:57:52.758145 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:57:52.758439 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:57:52.758651 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:57:52.931267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9220d0>]}
[0m23:57:52.932327 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:52.932822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:52.933925 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:57:52.934692 [info ] [MainThread]: 
[0m23:57:52.941641 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:57:52.942259 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:57:52.943365 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:57:52.943769 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:57:52.947198 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:52.948520 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:57:52.944080 => 2023-02-21 23:57:52.948453
[0m23:57:52.948850 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:57:52.963422 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:52.963685 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:52.963915 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:57:52.964108 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:57:53.585019 [debug] [Thread-1  ]: SQL status: OK in 0.62 seconds
[0m23:57:53.634213 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:53.635675 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:53.635890 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:57:54.218335 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:57:54.219596 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.220107 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:57:54.220521 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\x8eF\x1f\x84\xa2\rC\x15\x7f]\xf4!'
[0m23:57:54.221481 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:57:52.949097 => 2023-02-21 23:57:54.221316
[0m23:57:54.222060 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:57:54.222506 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.222910 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:57:54.412609 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.412960 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9a3370>]}
[0m23:57:54.413306 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.47s]
[0m23:57:54.414476 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:57:54.415480 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:54.415721 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:54.415963 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:54.592239 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.593285 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:54.593797 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:54.594303 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:54.594747 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.595181 [debug] [MainThread]: On master: Close
[0m23:57:54.755197 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:54.756207 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:57:54.759000 [info ] [MainThread]: 
[0m23:57:54.759786 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.98 seconds (3.98s).
[0m23:57:54.760727 [debug] [MainThread]: Command end result
[0m23:57:54.772390 [info ] [MainThread]: 
[0m23:57:54.773023 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:57:54.773472 [info ] [MainThread]: 
[0m23:57:54.773845 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:57:54.774294 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.774799 [info ] [MainThread]: 
[0m23:57:54.775281 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:57:54.775795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca9f6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c983bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cd5dc40>]}
[0m23:57:54.776175 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:59:39.691600 | f1f0f95f-7296-4e65-989d-9517a2efbab0 ==============================
[0m23:59:39.691600 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:59:39.693604 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:59:39.694355 [debug] [MainThread]: Tracking: tracking
[0m23:59:39.707470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126616160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126616f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1266165e0>]}
[0m23:59:39.761444 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:59:39.761715 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:59:39.769046 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:59:39.780823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12677fd60>]}
[0m23:59:39.784577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10600be80>]}
[0m23:59:39.784765 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:59:39.784980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053ca670>]}
[0m23:59:39.785511 [info ] [MainThread]: 
[0m23:59:39.786358 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:59:39.786833 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:59:39.792171 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:59:39.792412 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:59:39.792570 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:59:40.486522 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
[0m23:59:40.500643 [debug] [ThreadPool]: On list_schemas: Close
[0m23:59:40.656428 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:59:40.673781 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:40.674124 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:59:40.674412 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:59:40.674713 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:59:41.284888 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:59:41.300412 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:59:41.300782 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:59:41.739698 [debug] [ThreadPool]: SQL status: OK in 0.44 seconds
[0m23:59:41.746082 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:59:41.746713 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:59:41.747143 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:59:41.902161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268234f0>]}
[0m23:59:41.903430 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:41.903929 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:59:41.905089 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:59:41.905849 [info ] [MainThread]: 
[0m23:59:41.912864 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:59:41.913491 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:59:41.914569 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:59:41.914960 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:59:41.918435 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:59:41.919859 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:59:41.915272 => 2023-02-21 23:59:41.919783
[0m23:59:41.920209 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:59:41.935034 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:41.935333 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:59:41.935582 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:59:41.935781 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:59:42.567258 [debug] [Thread-1  ]: SQL status: OK in 0.63 seconds
[0m23:59:42.617066 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:59:42.617522 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:59:42.617724 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.*, dbx.* from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:59:43.269839 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.*, dbx.* from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:59:43.270757 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.271254 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:59:43.271689 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xcf<\x10\xaf\x9c\xf8A\t\xf1h\xf0\xb1'
[0m23:59:43.272588 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:59:41.920462 => 2023-02-21 23:59:43.272417
[0m23:59:43.273107 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:59:43.273525 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.273929 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:59:43.462335 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.462774 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a2370>]}
[0m23:59:43.463191 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.55s]
[0m23:59:43.464539 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:59:43.465535 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:59:43.465753 [debug] [MainThread]: On master: ROLLBACK
[0m23:59:43.465950 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:59:43.642518 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.643359 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:43.643821 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:59:43.644307 [debug] [MainThread]: On master: ROLLBACK
[0m23:59:43.644772 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.645217 [debug] [MainThread]: On master: Close
[0m23:59:43.800513 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:59:43.801653 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:59:43.803991 [info ] [MainThread]: 
[0m23:59:43.804754 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.02 seconds (4.02s).
[0m23:59:43.805672 [debug] [MainThread]: Command end result
[0m23:59:43.817831 [info ] [MainThread]: 
[0m23:59:43.818463 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:59:43.818990 [info ] [MainThread]: 
[0m23:59:43.819383 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:59:43.819732 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.820107 [info ] [MainThread]: 
[0m23:59:43.820523 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:59:43.821024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264965b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a2370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268b0ee0>]}
[0m23:59:43.821381 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:00:01.941349 | 0e10869a-7fd6-49d4-b965-8278341dd1ef ==============================
[0m00:00:01.941349 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:00:01.947851 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:00:01.948204 [debug] [MainThread]: Tracking: tracking
[0m00:00:01.958487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856880>]}
[0m00:00:02.018108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:02.018387 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:00:02.025570 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:00:02.037397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1509bbb80>]}
[0m00:00:02.041167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1257b2310>]}
[0m00:00:02.041355 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:00:02.041569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064ce550>]}
[0m00:00:02.042082 [info ] [MainThread]: 
[0m00:00:02.042909 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:02.043394 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:00:02.049037 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:00:02.049282 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:00:02.049427 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:02.699004 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m00:00:02.711538 [debug] [ThreadPool]: On list_schemas: Close
[0m00:00:02.875858 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:00:02.892596 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:02.892964 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:02.893255 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:00:02.893513 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:00:03.520081 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m00:00:03.538111 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:03.538472 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:00:04.031594 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m00:00:04.038267 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:00:04.038964 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:00:04.039403 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:00:04.197857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150b62490>]}
[0m00:00:04.199067 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:04.199685 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:04.200897 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:00:04.201737 [info ] [MainThread]: 
[0m00:00:04.208990 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:00:04.209493 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:00:04.210545 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:00:04.210946 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:00:04.214501 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:00:04.215180 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:00:04.211251 => 2023-02-22 00:00:04.215104
[0m00:00:04.215578 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:00:04.230948 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:04.231223 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:00:04.231451 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:00:04.231650 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:00:04.952410 [debug] [Thread-1  ]: SQL status: OK in 0.72 seconds
[0m00:00:05.003673 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:00:05.004129 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:00:05.004336 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id, dbx.id from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:00:05.567615 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id, dbx.id from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:00:05.568991 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:05.569745 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:00:05.570115 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xdc\x95\x1b\x9b\x99\xb1%\xe5\n\x00\x02\x15'
[0m00:00:05.570933 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:00:04.215859 => 2023-02-22 00:00:05.570770
[0m00:00:05.571420 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:00:05.571778 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.572129 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:00:05.770620 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:05.771126 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150be3370>]}
[0m00:00:05.771585 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.56s]
[0m00:00:05.772924 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:00:05.773920 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:05.774170 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:05.774378 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:00:05.951745 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.952730 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:05.953179 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:05.953605 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:05.953963 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.954306 [debug] [MainThread]: On master: Close
[0m00:00:06.104460 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:00:06.105587 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:00:06.108474 [info ] [MainThread]: 
[0m00:00:06.109251 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.07 seconds (4.07s).
[0m00:00:06.110179 [debug] [MainThread]: Command end result
[0m00:00:06.121896 [info ] [MainThread]: 
[0m00:00:06.122596 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:00:06.123134 [info ] [MainThread]: 
[0m00:00:06.123545 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:00:06.123911 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:06.124334 [info ] [MainThread]: 
[0m00:00:06.124759 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:00:06.125273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150c53940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1257b2250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150bc2b20>]}
[0m00:00:06.125674 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:00:58.709033 | 4073598b-7921-40c4-8b10-e826d6b1d07e ==============================
[0m00:00:58.709033 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:00:58.711921 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:00:58.712113 [debug] [MainThread]: Tracking: tracking
[0m00:00:58.731926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad7160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad7f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad75e0>]}
[0m00:00:58.791510 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:58.791787 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:00:58.799110 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:00:58.810783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130e3a400>]}
[0m00:00:58.814401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10850fe80>]}
[0m00:00:58.814583 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:00:58.814811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078ce670>]}
[0m00:00:58.815326 [info ] [MainThread]: 
[0m00:00:58.816150 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:58.816618 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:00:58.822060 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:00:58.822320 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:00:58.822454 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:59.541798 [debug] [ThreadPool]: SQL status: OK in 0.72 seconds
[0m00:00:59.557850 [debug] [ThreadPool]: On list_schemas: Close
[0m00:00:59.802362 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:00:59.819496 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:59.819801 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:59.820016 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:00:59.820218 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:00.453818 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m00:01:00.470997 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:00.471343 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:01:00.965020 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m00:01:00.971270 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:01:00.971896 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:01:00.972321 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:01:01.135905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ee24f0>]}
[0m00:01:01.137122 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.137618 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:01.138799 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:01:01.139507 [info ] [MainThread]: 
[0m00:01:01.146341 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:01:01.146957 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:01:01.147977 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:01:01.148266 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:01:01.151709 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.152655 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:01:01.148491 => 2023-02-22 00:01:01.152596
[0m00:01:01.152986 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:01:01.164718 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.164942 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.165100 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:01:01.165237 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:01:01.844049 [debug] [Thread-1  ]: SQL status: OK in 0.68 seconds
[0m00:01:01.895508 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.896010 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.896237 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:02.144662 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:02.145430 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

[0m00:01:02.145851 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:01:02.146243 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xfe~\x12\xc0\xadVdo(\x98,\x1f'
[0m00:01:02.147033 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:01:01.153200 => 2023-02-22 00:01:02.146920
[0m00:01:02.147560 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:01:02.147970 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.148368 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:01:02.335261 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  
    
      
          create or replace table `airbyte`.`gold_users_cities_join`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  -- CREATE TABLE snowflake_silver_users
  -- USING snowflake
  -- OPTIONS (
  --     dbtable 'silver_users',
  --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
  --     sfUser 'jwszolek',
  --     sfPassword '.8kki-cqP@f2W@7zYY46',
  --     sfDatabase 'jwszol',
  --     sfSchema 'public',
  --     sfWarehouse 'COMPUTE_WH'
  -- );
  
  select snow.id as 'snow_id', 
  ------------------^^^
         dbx.id as 'dbx_id' 
  from hive_metastore.default.snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.id == dbx.user_id
  
[0m00:01:02.335696 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f63370>]}
[0m00:01:02.336071 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.19s]
[0m00:01:02.337304 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:01:02.338342 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:02.338594 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:02.338802 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:02.495238 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.496240 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:02.496694 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:02.497171 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:02.497610 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.498042 [debug] [MainThread]: On master: Close
[0m00:01:02.662725 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:02.664161 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:01:02.666227 [info ] [MainThread]: 
[0m00:01:02.666914 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.85 seconds (3.85s).
[0m00:01:02.667601 [debug] [MainThread]: Command end result
[0m00:01:02.677143 [info ] [MainThread]: 
[0m00:01:02.677636 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:01:02.678053 [info ] [MainThread]: 
[0m00:01:02.678509 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:01:02.678889 [error] [MainThread]:   
[0m00:01:02.679267 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)
[0m00:01:02.679666 [error] [MainThread]:   
[0m00:01:02.680052 [error] [MainThread]:   == SQL ==
[0m00:01:02.680440 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m00:01:02.680837 [error] [MainThread]:   
[0m00:01:02.681218 [error] [MainThread]:     
[0m00:01:02.681600 [error] [MainThread]:       
[0m00:01:02.681960 [error] [MainThread]:           create or replace table `airbyte`.`gold_users_cities_join`
[0m00:01:02.682288 [error] [MainThread]:         
[0m00:01:02.682615 [error] [MainThread]:         
[0m00:01:02.682940 [error] [MainThread]:       using delta
[0m00:01:02.683272 [error] [MainThread]:         
[0m00:01:02.683618 [error] [MainThread]:         
[0m00:01:02.683916 [error] [MainThread]:         
[0m00:01:02.684166 [error] [MainThread]:         
[0m00:01:02.684437 [error] [MainThread]:         
[0m00:01:02.684761 [error] [MainThread]:         
[0m00:01:02.685088 [error] [MainThread]:         as
[0m00:01:02.685424 [error] [MainThread]:         
[0m00:01:02.685773 [error] [MainThread]:   
[0m00:01:02.686101 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m00:01:02.686431 [error] [MainThread]:   -- CREATE TABLE snowflake_silver_users
[0m00:01:02.686761 [error] [MainThread]:   -- USING snowflake
[0m00:01:02.687087 [error] [MainThread]:   -- OPTIONS (
[0m00:01:02.687411 [error] [MainThread]:   --     dbtable 'silver_users',
[0m00:01:02.687743 [error] [MainThread]:   --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m00:01:02.688079 [error] [MainThread]:   --     sfUser 'jwszolek',
[0m00:01:02.688416 [error] [MainThread]:   --     sfPassword '.8kki-cqP@f2W@7zYY46',
[0m00:01:02.688743 [error] [MainThread]:   --     sfDatabase 'jwszol',
[0m00:01:02.689086 [error] [MainThread]:   --     sfSchema 'public',
[0m00:01:02.689404 [error] [MainThread]:   --     sfWarehouse 'COMPUTE_WH'
[0m00:01:02.689726 [error] [MainThread]:   -- );
[0m00:01:02.690002 [error] [MainThread]:   
[0m00:01:02.690279 [error] [MainThread]:   select snow.id as 'snow_id', 
[0m00:01:02.690557 [error] [MainThread]:   ------------------^^^
[0m00:01:02.690834 [error] [MainThread]:          dbx.id as 'dbx_id' 
[0m00:01:02.691131 [error] [MainThread]:   from hive_metastore.default.snowflake_silver_users snow
[0m00:01:02.691411 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.id == dbx.user_id
[0m00:01:02.691690 [error] [MainThread]:   
[0m00:01:02.691983 [info ] [MainThread]: 
[0m00:01:02.692283 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:01:02.692641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f512b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130a5e5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f70ee0>]}
[0m00:01:02.692901 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:01:20.027768 | 25a5f8b4-0b85-4a5a-b0f3-979f954b5979 ==============================
[0m00:01:20.027768 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:01:20.029314 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:01:20.029505 [debug] [MainThread]: Tracking: tracking
[0m00:01:20.049970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cf190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cffa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cf610>]}
[0m00:01:20.103116 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:01:20.103392 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:01:20.112392 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:01:20.125033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12743a310>]}
[0m00:01:20.128611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12715b6a0>]}
[0m00:01:20.128797 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:01:20.129053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b4b50>]}
[0m00:01:20.129613 [info ] [MainThread]: 
[0m00:01:20.130627 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:20.131106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:01:20.136770 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:01:20.137002 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:01:20.137139 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:20.736069 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m00:01:20.749552 [debug] [ThreadPool]: On list_schemas: Close
[0m00:01:20.902165 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:01:20.919413 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:20.919806 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:20.920126 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:01:20.920422 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:21.499671 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m00:01:21.514754 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:21.515137 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:01:21.915204 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m00:01:21.922988 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:01:21.923643 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:01:21.924076 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:01:22.075031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1274e7f40>]}
[0m00:01:22.076511 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:22.077020 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:22.078143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:01:22.078806 [info ] [MainThread]: 
[0m00:01:22.085903 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:01:22.086519 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:01:22.087330 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:01:22.087613 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:01:22.091272 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.091992 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:01:22.087877 => 2023-02-22 00:01:22.091930
[0m00:01:22.092282 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:01:22.106976 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:22.107244 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.107472 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:01:22.107678 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:01:22.750039 [debug] [Thread-1  ]: SQL status: OK in 0.64 seconds
[0m00:01:22.801636 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.802154 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.802380 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:23.039713 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:23.040236 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

[0m00:01:23.040569 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:01:23.040893 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2D\n\xf2\x15\x13\x83\x8aD\xb7\xfe\x85y\xb2'
[0m00:01:23.041480 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:01:22.092487 => 2023-02-22 00:01:23.041398
[0m00:01:23.041826 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:01:23.042114 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.042395 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:01:23.239831 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  
    
      
          create or replace table `airbyte`.`gold_users_cities_join`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  -- CREATE TABLE snowflake_silver_users
  -- USING snowflake
  -- OPTIONS (
  --     dbtable 'silver_users',
  --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
  --     sfUser 'jwszolek',
  --     sfPassword '.8kki-cqP@f2W@7zYY46',
  --     sfDatabase 'jwszol',
  --     sfSchema 'public',
  --     sfWarehouse 'COMPUTE_WH'
  -- );
  
  select snow.id 'snow_id', 
  ---------------^^^
         dbx.id 'dbx_id' 
  from hive_metastore.default.snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.id == dbx.user_id
  
[0m00:01:23.240242 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127562370>]}
[0m00:01:23.240605 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.15s]
[0m00:01:23.241819 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:01:23.242813 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:23.243062 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:23.243260 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:23.425277 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.426418 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:23.426886 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:23.427378 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:23.427799 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.428172 [debug] [MainThread]: On master: Close
[0m00:01:23.606621 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:23.607183 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:01:23.610496 [info ] [MainThread]: 
[0m00:01:23.611217 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.48 seconds (3.48s).
[0m00:01:23.611971 [debug] [MainThread]: Command end result
[0m00:01:23.625177 [info ] [MainThread]: 
[0m00:01:23.625696 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:01:23.626058 [info ] [MainThread]: 
[0m00:01:23.626828 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:01:23.627370 [error] [MainThread]:   
[0m00:01:23.627934 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)
[0m00:01:23.628444 [error] [MainThread]:   
[0m00:01:23.628901 [error] [MainThread]:   == SQL ==
[0m00:01:23.629392 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m00:01:23.629919 [error] [MainThread]:   
[0m00:01:23.630479 [error] [MainThread]:     
[0m00:01:23.630926 [error] [MainThread]:       
[0m00:01:23.631234 [error] [MainThread]:           create or replace table `airbyte`.`gold_users_cities_join`
[0m00:01:23.631721 [error] [MainThread]:         
[0m00:01:23.632129 [error] [MainThread]:         
[0m00:01:23.632836 [error] [MainThread]:       using delta
[0m00:01:23.633494 [error] [MainThread]:         
[0m00:01:23.634321 [error] [MainThread]:         
[0m00:01:23.634961 [error] [MainThread]:         
[0m00:01:23.635710 [error] [MainThread]:         
[0m00:01:23.636314 [error] [MainThread]:         
[0m00:01:23.636871 [error] [MainThread]:         
[0m00:01:23.637213 [error] [MainThread]:         as
[0m00:01:23.637786 [error] [MainThread]:         
[0m00:01:23.638545 [error] [MainThread]:   
[0m00:01:23.639211 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m00:01:23.639678 [error] [MainThread]:   -- CREATE TABLE snowflake_silver_users
[0m00:01:23.640131 [error] [MainThread]:   -- USING snowflake
[0m00:01:23.640756 [error] [MainThread]:   -- OPTIONS (
[0m00:01:23.641277 [error] [MainThread]:   --     dbtable 'silver_users',
[0m00:01:23.641784 [error] [MainThread]:   --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m00:01:23.642387 [error] [MainThread]:   --     sfUser 'jwszolek',
[0m00:01:23.642852 [error] [MainThread]:   --     sfPassword '.8kki-cqP@f2W@7zYY46',
[0m00:01:23.643305 [error] [MainThread]:   --     sfDatabase 'jwszol',
[0m00:01:23.643751 [error] [MainThread]:   --     sfSchema 'public',
[0m00:01:23.644196 [error] [MainThread]:   --     sfWarehouse 'COMPUTE_WH'
[0m00:01:23.644640 [error] [MainThread]:   -- );
[0m00:01:23.645078 [error] [MainThread]:   
[0m00:01:23.645516 [error] [MainThread]:   select snow.id 'snow_id', 
[0m00:01:23.645975 [error] [MainThread]:   ---------------^^^
[0m00:01:23.646423 [error] [MainThread]:          dbx.id 'dbx_id' 
[0m00:01:23.646893 [error] [MainThread]:   from hive_metastore.default.snowflake_silver_users snow
[0m00:01:23.647388 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.id == dbx.user_id
[0m00:01:23.647660 [error] [MainThread]:   
[0m00:01:23.647972 [info ] [MainThread]: 
[0m00:01:23.648621 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:01:23.649314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12715b6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127542bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12741dc40>]}
[0m00:01:23.649663 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:02:11.031137 | 3cb1975b-c53a-40ea-a2db-64bc13dc9442 ==============================
[0m00:02:11.031137 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:02:11.032773 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:02:11.032968 [debug] [MainThread]: Tracking: tracking
[0m00:02:11.044197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d71c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d7640>]}
[0m00:02:11.097205 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:02:11.097488 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:02:11.104809 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:02:11.116668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e33fac0>]}
[0m00:02:11.120444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107923af0>]}
[0m00:02:11.120633 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:02:11.120848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107935df0>]}
[0m00:02:11.121412 [info ] [MainThread]: 
[0m00:02:11.122256 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:02:11.122741 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:02:11.128172 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:02:11.128406 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:02:11.128536 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:11.806900 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m00:02:11.821541 [debug] [ThreadPool]: On list_schemas: Close
[0m00:02:11.995228 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:02:12.013937 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:12.014398 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:02:12.014720 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:02:12.015030 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:12.596178 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m00:02:12.613803 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:02:12.614201 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:02:13.083690 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
[0m00:02:13.088897 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:02:13.089346 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:02:13.089612 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:02:13.263271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e3e4070>]}
[0m00:02:13.264370 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:13.264856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:13.265971 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:02:13.266704 [info ] [MainThread]: 
[0m00:02:13.274053 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:02:13.274638 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:02:13.275682 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:02:13.276005 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:02:13.279434 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.280566 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:02:13.276220 => 2023-02-22 00:02:13.280498
[0m00:02:13.280925 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:02:13.295783 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:13.296040 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.296262 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:02:13.296457 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:02:13.940891 [debug] [Thread-1  ]: SQL status: OK in 0.64 seconds
[0m00:02:13.992957 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.994315 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.994536 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       dbx.id as dbx_id 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:02:29.766381 [debug] [Thread-1  ]: SQL status: OK in 15.77 seconds
[0m00:02:30.063628 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:02:13.281202 => 2023-02-22 00:02:30.063512
[0m00:02:30.064166 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:02:30.064481 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.064725 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:02:30.217533 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e462370>]}
[0m00:02:30.219258 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 16.94s]
[0m00:02:30.222798 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:02:30.226436 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:02:30.227059 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:30.227487 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:02:30.414895 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.416162 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:30.416634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:30.417140 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:30.417586 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.418020 [debug] [MainThread]: On master: Close
[0m00:02:30.568441 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:02:30.569408 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:02:30.571811 [info ] [MainThread]: 
[0m00:02:30.572832 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 19.45 seconds (19.45s).
[0m00:02:30.573986 [debug] [MainThread]: Command end result
[0m00:02:30.588584 [info ] [MainThread]: 
[0m00:02:30.589177 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:02:30.589632 [info ] [MainThread]: 
[0m00:02:30.590090 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:02:30.590668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b6190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b6d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b62b0>]}
[0m00:02:30.591054 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:03:53.611512 | f7562c22-0376-453d-9ee4-16759332ce4a ==============================
[0m00:03:53.611512 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:03:53.613196 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:03:53.613366 [debug] [MainThread]: Tracking: tracking
[0m00:03:53.625016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757610>]}
[0m00:03:53.688555 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:03:53.688736 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:03:53.692192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1248dbf40>]}
[0m00:03:53.696400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124820c10>]}
[0m00:03:53.696588 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:03:53.696800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c11b50>]}
[0m00:03:53.697351 [info ] [MainThread]: 
[0m00:03:53.698268 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:03:53.698795 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:03:53.704689 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:03:53.704942 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:03:53.705081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:03:54.347647 [debug] [ThreadPool]: SQL status: OK in 0.64 seconds
[0m00:03:54.364292 [debug] [ThreadPool]: On list_schemas: Close
[0m00:03:54.536441 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:03:54.560785 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:54.561316 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:03:54.561709 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:03:54.561975 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:03:55.252796 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
[0m00:03:55.273719 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:03:55.274457 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:03:55.843744 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
[0m00:03:55.853459 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:03:55.854246 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:03:55.854774 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:03:56.007526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124820e20>]}
[0m00:03:56.009276 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:56.010005 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:03:56.011691 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:03:56.012772 [info ] [MainThread]: 
[0m00:03:56.021053 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:03:56.021928 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:03:56.023154 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:03:56.023509 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:03:56.027836 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.028868 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:03:56.023743 => 2023-02-22 00:03:56.028803
[0m00:03:56.029174 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:03:56.079967 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.082450 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:56.082641 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.082825 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       dbx.id as dbx_id 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:03:56.082971 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:04:02.793026 [debug] [Thread-1  ]: SQL status: OK in 6.71 seconds
[0m00:04:03.072876 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:03:56.029408 => 2023-02-22 00:04:03.072801
[0m00:04:03.073324 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:04:03.073592 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.073837 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:04:03.249098 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249694c0>]}
[0m00:04:03.250383 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 7.23s]
[0m00:04:03.254521 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:04:03.257377 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:04:03.257944 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.258381 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:04:03.451181 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.452766 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:03.453648 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:04:03.454563 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.455085 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.455484 [debug] [MainThread]: On master: Close
[0m00:04:03.598085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:04:03.598778 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:04:03.601110 [info ] [MainThread]: 
[0m00:04:03.601872 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.90 seconds (9.90s).
[0m00:04:03.602777 [debug] [MainThread]: Command end result
[0m00:04:03.615419 [info ] [MainThread]: 
[0m00:04:03.615979 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:04:03.616420 [info ] [MainThread]: 
[0m00:04:03.616838 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:04:03.617407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249659a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124999160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124a03790>]}
[0m00:04:03.617770 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:06:09.342524 | 511883b0-2558-4d11-b233-5f3bfc54e695 ==============================
[0m00:06:09.342524 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:06:09.344249 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:06:09.344494 [debug] [MainThread]: Tracking: tracking
[0m00:06:09.356349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba931f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba93f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba93670>]}
[0m00:06:09.418635 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:06:09.418908 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:06:09.427433 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:06:09.439348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcfaa90>]}
[0m00:06:09.442927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10501bd90>]}
[0m00:06:09.443109 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:06:09.443329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c27e50>]}
[0m00:06:09.443865 [info ] [MainThread]: 
[0m00:06:09.444734 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:06:09.445250 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:06:09.450955 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:06:09.451197 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:06:09.451348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:06:10.021065 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
[0m00:06:10.037765 [debug] [ThreadPool]: On list_schemas: Close
[0m00:06:10.255868 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:06:10.279795 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:10.280188 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:06:10.280464 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:06:10.280722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:06:10.866323 [debug] [ThreadPool]: SQL status: OK in 0.59 seconds
[0m00:06:10.888937 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:06:10.889373 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:06:11.304545 [debug] [ThreadPool]: SQL status: OK in 0.41 seconds
[0m00:06:11.312605 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:06:11.313496 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:06:11.313977 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:06:11.472634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bda2790>]}
[0m00:06:11.474159 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:11.474915 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:06:11.476230 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:06:11.477232 [info ] [MainThread]: 
[0m00:06:11.485128 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:06:11.486187 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:06:11.487698 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:06:11.488162 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:06:11.493209 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.494540 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:06:11.488484 => 2023-02-22 00:06:11.494438
[0m00:06:11.495020 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:06:11.543038 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.543723 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:11.543935 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.544129 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       snow.name as snow_name,
       dbx.id as dbx_id,
       dbx.name as dbx_name,
       dbx.user_id as dbx_user_id
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:06:11.544290 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:06:18.977448 [debug] [Thread-1  ]: SQL status: OK in 7.43 seconds
[0m00:06:19.271848 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:06:11.495345 => 2023-02-22 00:06:19.271749
[0m00:06:19.272338 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:06:19.272634 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.272880 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:06:19.452249 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf23370>]}
[0m00:06:19.453998 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 7.96s]
[0m00:06:19.457614 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:06:19.461338 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:06:19.462097 [debug] [MainThread]: On master: ROLLBACK
[0m00:06:19.462542 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:06:19.637785 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.638983 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:19.639475 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:06:19.639956 [debug] [MainThread]: On master: ROLLBACK
[0m00:06:19.640412 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.640855 [debug] [MainThread]: On master: Close
[0m00:06:19.802214 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:06:19.803303 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:06:19.805929 [info ] [MainThread]: 
[0m00:06:19.806955 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.36 seconds (10.36s).
[0m00:06:19.807980 [debug] [MainThread]: Command end result
[0m00:06:19.820696 [info ] [MainThread]: 
[0m00:06:19.821385 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:06:19.821845 [info ] [MainThread]: 
[0m00:06:19.822252 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:06:19.822870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf30f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf04a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcde100>]}
[0m00:06:19.823301 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:10:33.289360 | bcf8684b-313c-4a1f-8335-a9ee96408058 ==============================
[0m00:10:33.289360 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:10:33.291013 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:10:33.291914 [debug] [MainThread]: Tracking: tracking
[0m00:10:33.312198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9ea160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9eaf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9ea880>]}
[0m00:10:33.373440 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:10:33.373714 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:10:33.373883 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities.sql
[0m00:10:33.382541 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:10:33.389533 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m00:10:33.396125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab46040>]}
[0m00:10:33.399717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab2efd0>]}
[0m00:10:33.399900 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:10:33.400115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e2d250>]}
[0m00:10:33.400639 [info ] [MainThread]: 
[0m00:10:33.401535 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:10:33.402066 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:10:33.407493 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:10:33.407720 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:10:33.407847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:10:34.218614 [debug] [ThreadPool]: SQL status: OK in 0.81 seconds
[0m00:10:34.230557 [debug] [ThreadPool]: On list_schemas: Close
[0m00:10:34.386947 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:10:34.403858 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:34.404230 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:10:34.404515 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:10:34.404869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:10:35.086501 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m00:10:35.099995 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:10:35.100431 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:10:35.519091 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m00:10:35.524562 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:10:35.525187 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:10:35.525608 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:10:35.685820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abb2a90>]}
[0m00:10:35.695192 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:35.695850 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:10:35.714302 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:10:35.715778 [info ] [MainThread]: 
[0m00:10:35.737770 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m00:10:35.738342 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities ............................. [RUN]
[0m00:10:35.739086 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m00:10:35.739315 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m00:10:35.741550 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m00:10:35.742323 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-22 00:10:35.739486 => 2023-02-22 00:10:35.742262
[0m00:10:35.742523 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m00:10:35.752571 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:35.752807 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m00:10:35.753005 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

      describe extended `airbyte`.`silver_cities`
  
[0m00:10:35.753173 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:10:36.460791 [debug] [Thread-1  ]: SQL status: OK in 0.71 seconds
[0m00:10:36.506282 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m00:10:36.506797 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m00:10:36.506998 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `airbyte`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 6
  
[0m00:10:40.131825 [debug] [Thread-1  ]: SQL status: OK in 3.62 seconds
[0m00:10:40.158175 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-22 00:10:35.742654 => 2023-02-22 00:10:40.158101
[0m00:10:40.158613 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m00:10:40.158857 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.159064 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m00:10:40.319458 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ada3370>]}
[0m00:10:40.320835 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities ........................ [[32mOK[0m in 4.58s]
[0m00:10:40.323537 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m00:10:40.325472 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:10:40.325964 [debug] [MainThread]: On master: ROLLBACK
[0m00:10:40.326351 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:10:40.507174 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.508366 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:40.509228 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:10:40.510068 [debug] [MainThread]: On master: ROLLBACK
[0m00:10:40.510583 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.510939 [debug] [MainThread]: On master: Close
[0m00:10:40.664144 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:10:40.665191 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m00:10:40.668021 [info ] [MainThread]: 
[0m00:10:40.668677 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.27 seconds (7.27s).
[0m00:10:40.669580 [debug] [MainThread]: Command end result
[0m00:10:40.681385 [info ] [MainThread]: 
[0m00:10:40.681816 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:10:40.682183 [info ] [MainThread]: 
[0m00:10:40.682602 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:10:40.683113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036a2f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf5220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf5280>]}
[0m00:10:40.683464 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:12:59.089705 | 67fb0201-24e8-451f-b812-389a1382a758 ==============================
[0m00:12:59.089705 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:12:59.091527 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_users'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:12:59.091781 [debug] [MainThread]: Tracking: tracking
[0m00:12:59.103613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828610>]}
[0m00:12:59.157611 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:12:59.157799 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:12:59.161331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152adbf40>]}
[0m00:12:59.165285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1ac10>]}
[0m00:12:59.165480 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:12:59.165694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107160100>]}
[0m00:12:59.166012 [warn ] [MainThread]: The selection criterion 'silver_users' does not match any nodes
[0m00:12:59.166482 [info ] [MainThread]: 
[0m00:12:59.166742 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m00:12:59.166959 [debug] [MainThread]: Command end result
[0m00:12:59.170163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10715f8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1a9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1a100>]}
[0m00:12:59.170321 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:13:07.469327 | 2a334cc2-e262-467e-8576-df2fde2000cc ==============================
[0m00:13:07.469327 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:13:07.471492 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:13:07.471764 [debug] [MainThread]: Tracking: tracking
[0m00:13:07.487455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9971c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d997fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d997640>]}
[0m00:13:07.544828 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:13:07.545017 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:13:07.548458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11db1cf40>]}
[0m00:13:07.552136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da5bc40>]}
[0m00:13:07.552331 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:13:07.552547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a5df0>]}
[0m00:13:07.553075 [info ] [MainThread]: 
[0m00:13:07.553925 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:07.554412 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:13:07.560259 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:13:07.560516 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:13:07.560661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:13:08.426546 [debug] [ThreadPool]: SQL status: OK in 0.87 seconds
[0m00:13:08.444983 [debug] [ThreadPool]: On list_schemas: Close
[0m00:13:08.677035 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:13:08.697973 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:08.698361 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:13:08.698642 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:13:08.698897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:13:09.344534 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m00:13:09.358662 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:13:09.359062 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:13:09.775164 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m00:13:09.781887 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:13:09.782509 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:13:09.782940 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:13:09.968209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da5be50>]}
[0m00:13:09.969271 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:09.969744 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:09.970646 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:13:09.971222 [info ] [MainThread]: 
[0m00:13:09.976414 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:13:09.976909 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:13:09.977839 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:13:09.978149 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:13:09.981212 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:13:09.982568 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:13:09.978368 => 2023-02-22 00:13:09.982503
[0m00:13:09.982903 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:13:09.996771 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:09.996951 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:13:09.997131 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:13:09.997283 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:13:10.678809 [debug] [Thread-1  ]: SQL status: OK in 0.68 seconds
[0m00:13:10.722726 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:13:10.723545 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:13:10.723758 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select snow.id as snow_id, 
       snow.name as snow_name,
       dbx.id as dbx_id,
       dbx.name as dbx_name,
       dbx.user_id as dbx_user_id
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:13:19.343147 [debug] [Thread-1  ]: SQL status: OK in 8.62 seconds
[0m00:13:19.603409 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:13:09.983156 => 2023-02-22 00:13:19.603344
[0m00:13:19.603834 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:13:19.604072 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.604278 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:13:19.750951 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df8c100>]}
[0m00:13:19.752323 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 9.77s]
[0m00:13:19.755478 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:13:19.758200 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:19.758867 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:19.759456 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:13:19.951674 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.953323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:19.954067 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:19.954478 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:19.954833 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.955181 [debug] [MainThread]: On master: Close
[0m00:13:20.105972 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:13:20.106929 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:13:20.109043 [info ] [MainThread]: 
[0m00:13:20.109913 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.56 seconds (12.56s).
[0m00:13:20.110673 [debug] [MainThread]: Command end result
[0m00:13:20.120355 [info ] [MainThread]: 
[0m00:13:20.120904 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:13:20.121337 [info ] [MainThread]: 
[0m00:13:20.121720 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:13:20.122192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a5df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbcb520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df13fa0>]}
[0m00:13:20.122560 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:29:41.457786 | b3160c84-9053-47bd-b6c3-5b8b2e089dc0 ==============================
[0m19:29:41.457786 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:29:41.459164 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'parse_only': False, 'select': ['silver_cities_limit'], 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m19:29:41.459365 [debug] [MainThread]: Tracking: tracking
[0m19:29:41.473723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7800d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7b1220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7b1640>]}
[0m19:29:41.509526 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 1 files changed.
[0m19:29:41.509791 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/silver_cities_limit.sql
[0m19:29:41.509944 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/silver_users_count_vw.sql
[0m19:29:41.510124 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities.sql
[0m19:29:41.517027 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:29:41.525297 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:29:41.526144 [debug] [MainThread]: 1699: static parser successfully parsed silver_users_count_vw.sql
[0m19:29:41.528140 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m19:29:41.534526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b3160c84-9053-47bd-b6c3-5b8b2e089dc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea000d0>]}
[0m19:29:41.538168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b3160c84-9053-47bd-b6c3-5b8b2e089dc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7fbee0>]}
[0m19:29:41.538368 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:29:41.538586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3160c84-9053-47bd-b6c3-5b8b2e089dc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9277f0>]}
[0m19:29:41.539186 [info ] [MainThread]: 
[0m19:29:41.540034 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:29:41.540598 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:29:41.548537 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:29:41.548845 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:29:41.549080 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:29:41.549230 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:29:42.686483 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
[0m19:29:42.709895 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:29:42.710177 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:29:43.242840 [debug] [ThreadPool]: SQL status: OK in 0.53 seconds
[0m19:29:43.249703 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:29:43.250310 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:29:43.250767 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:29:43.509937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3160c84-9053-47bd-b6c3-5b8b2e089dc0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7a72e0>]}
[0m19:29:43.511526 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:29:43.512386 [info ] [MainThread]: 
[0m19:29:43.519198 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:29:43.520126 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:29:43.520471 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:29:43.524726 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:29:43.525446 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:29:43.520749 => 2023-03-30 19:29:43.525364
[0m19:29:43.525783 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:29:43.526065 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:29:43.526017 => 2023-03-30 19:29:43.526033
[0m19:29:43.527863 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:29:43.528656 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:29:43.528933 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:29:43.529291 [debug] [MainThread]: Command end result
[0m19:29:43.535689 [info ] [MainThread]: Done.
[0m19:29:43.536115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056f0a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7a74f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8a5c70>]}
[0m19:29:43.536392 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:31:22.531325 | 3839546f-f538-451e-a17f-37a18a50ac67 ==============================
[0m19:31:22.531325 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:31:22.532743 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'parse_only': False, 'select': ['silver_cities_limit'], 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m19:31:22.532998 [debug] [MainThread]: Tracking: tracking
[0m19:31:22.542936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad6ff40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad95220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad95640>]}
[0m19:31:22.572778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:31:22.573084 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_limit.sql
[0m19:31:22.581511 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:31:22.589665 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:31:22.594755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3839546f-f538-451e-a17f-37a18a50ac67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b2250d0>]}
[0m19:31:22.598749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3839546f-f538-451e-a17f-37a18a50ac67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11addbac0>]}
[0m19:31:22.598948 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:31:22.599161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3839546f-f538-451e-a17f-37a18a50ac67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11addbe50>]}
[0m19:31:22.599800 [info ] [MainThread]: 
[0m19:31:22.600699 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:31:22.601319 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:31:22.609781 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:31:22.610067 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:31:22.610280 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:31:22.610420 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:23.656021 [debug] [ThreadPool]: SQL status: OK in 1.05 seconds
[0m19:31:23.679002 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:31:23.679258 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:31:24.365954 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
[0m19:31:24.370936 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:31:24.371410 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:31:24.371783 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:31:24.621192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3839546f-f538-451e-a17f-37a18a50ac67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b13fcd0>]}
[0m19:31:24.622385 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:24.622938 [info ] [MainThread]: 
[0m19:31:24.628496 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:31:24.629177 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:31:24.629478 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:31:24.633283 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:31:24.634086 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:31:24.629711 => 2023-03-30 19:31:24.634013
[0m19:31:24.634382 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:31:24.634648 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:31:24.634593 => 2023-03-30 19:31:24.634614
[0m19:31:24.636481 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:31:24.637175 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:24.637418 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:31:24.637721 [debug] [MainThread]: Command end result
[0m19:31:24.643752 [info ] [MainThread]: Done.
[0m19:31:24.644195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b2a4fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11addbe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ade1070>]}
[0m19:31:24.644478 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:32:52.566642 | 07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444 ==============================
[0m19:32:52.566642 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:32:52.568131 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_limit'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:32:52.568336 [debug] [MainThread]: Tracking: tracking
[0m19:32:52.577960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299cbc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299f0220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1299f0640>]}
[0m19:32:52.608041 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:32:52.608238 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:32:52.611992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129bbd0d0>]}
[0m19:32:52.616582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b8a280>]}
[0m19:32:52.616832 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:32:52.617051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b8a0d0>]}
[0m19:32:52.617757 [info ] [MainThread]: 
[0m19:32:52.618690 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:32:52.619231 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:32:52.625494 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:32:52.625789 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:32:52.625949 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:32:53.528652 [debug] [ThreadPool]: SQL status: OK in 0.9 seconds
[0m19:32:53.546595 [debug] [ThreadPool]: On list_schemas: Close
[0m19:32:53.800659 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:32:53.818589 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:32:53.818989 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:32:53.819303 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:32:53.819604 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:32:54.635048 [debug] [ThreadPool]: SQL status: OK in 0.82 seconds
[0m19:32:54.645818 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:32:54.646176 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:32:55.234254 [debug] [ThreadPool]: SQL status: OK in 0.59 seconds
[0m19:32:55.240417 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:32:55.240911 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:32:55.241285 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:32:55.391789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b81f40>]}
[0m19:32:55.392934 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:32:55.393445 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:32:55.394596 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:32:55.395300 [info ] [MainThread]: 
[0m19:32:55.402882 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:32:55.403525 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_limit ....................... [RUN]
[0m19:32:55.404623 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:32:55.405031 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:32:55.409515 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:32:55.410377 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:32:55.405351 => 2023-03-30 19:32:55.410288
[0m19:32:55.410728 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:32:55.456532 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_limit"
[0m19:32:55.456976 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:32:55.457144 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:32:55.457310 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 

    user_id = '20'

    user_id = '2'

    user_id = '19'

order by id
  
[0m19:32:55.457454 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:32:56.053226 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 

    user_id = '20'

    user_id = '2'

    user_id = '19'

order by id
  
[0m19:32:56.053972 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 

    user_id = '20'

    user_id = '2'
----^^^

    user_id = '19'

order by id
  

[0m19:32:56.054357 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 

    user_id = '20'

    user_id = '2'
----^^^

    user_id = '19'

order by id
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 

    user_id = '20'

    user_id = '2'
----^^^

    user_id = '19'

order by id
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m19:32:56.054711 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf1\xab\xaf\x11\xff\x99fvej\xbcm\xf6'
[0m19:32:56.055436 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:32:55.410977 => 2023-03-30 19:32:56.055283
[0m19:32:56.055886 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: ROLLBACK
[0m19:32:56.056201 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:32:56.056519 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: Close
[0m19:32:56.266427 [debug] [Thread-1  ]: Runtime Error in model silver_cities_limit (models/silver_cities_limit.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_limit`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select id, name, user_id from airbyte.bronze_cities
  where 
  
      user_id = '20'
  
      user_id = '2'
  ----^^^
  
      user_id = '19'
  
  order by id
    
  
[0m19:32:56.266780 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07b2f4c4-9ab7-4d2e-a5d3-bbae0dd2f444', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129c414f0>]}
[0m19:32:56.267135 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_limit .............. [[31mERROR[0m in 0.86s]
[0m19:32:56.268305 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:32:56.269242 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:32:56.269457 [debug] [MainThread]: On master: ROLLBACK
[0m19:32:56.269647 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:32:56.497879 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:32:56.499451 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:32:56.500318 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:32:56.501201 [debug] [MainThread]: On master: ROLLBACK
[0m19:32:56.502010 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:32:56.502840 [debug] [MainThread]: On master: Close
[0m19:32:56.650377 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:32:56.651533 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:32:56.654526 [info ] [MainThread]: 
[0m19:32:56.655427 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.04 seconds (4.04s).
[0m19:32:56.656570 [debug] [MainThread]: Command end result
[0m19:32:56.670323 [info ] [MainThread]: 
[0m19:32:56.670936 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:32:56.671364 [info ] [MainThread]: 
[0m19:32:56.671792 [error] [MainThread]: [33mRuntime Error in model silver_cities_limit (models/silver_cities_limit.sql)[0m
[0m19:32:56.672200 [error] [MainThread]:   
[0m19:32:56.672591 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)
[0m19:32:56.672977 [error] [MainThread]:   
[0m19:32:56.673358 [error] [MainThread]:   == SQL ==
[0m19:32:56.673741 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
[0m19:32:56.674124 [error] [MainThread]:   
[0m19:32:56.674503 [error] [MainThread]:     
[0m19:32:56.674882 [error] [MainThread]:       
[0m19:32:56.675261 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_limit`
[0m19:32:56.675643 [error] [MainThread]:         
[0m19:32:56.676019 [error] [MainThread]:         
[0m19:32:56.676393 [error] [MainThread]:       using delta
[0m19:32:56.676769 [error] [MainThread]:         
[0m19:32:56.677141 [error] [MainThread]:         
[0m19:32:56.677514 [error] [MainThread]:         
[0m19:32:56.677889 [error] [MainThread]:         
[0m19:32:56.678255 [error] [MainThread]:         
[0m19:32:56.678571 [error] [MainThread]:         
[0m19:32:56.678889 [error] [MainThread]:         as
[0m19:32:56.679212 [error] [MainThread]:         
[0m19:32:56.679537 [error] [MainThread]:   
[0m19:32:56.679869 [error] [MainThread]:   
[0m19:32:56.680207 [error] [MainThread]:   
[0m19:32:56.680538 [error] [MainThread]:   select id, name, user_id from airbyte.bronze_cities
[0m19:32:56.680882 [error] [MainThread]:   where 
[0m19:32:56.681224 [error] [MainThread]:   
[0m19:32:56.681558 [error] [MainThread]:       user_id = '20'
[0m19:32:56.681888 [error] [MainThread]:   
[0m19:32:56.682215 [error] [MainThread]:       user_id = '2'
[0m19:32:56.682542 [error] [MainThread]:   ----^^^
[0m19:32:56.682871 [error] [MainThread]:   
[0m19:32:56.683195 [error] [MainThread]:       user_id = '19'
[0m19:32:56.683514 [error] [MainThread]:   
[0m19:32:56.683831 [error] [MainThread]:   order by id
[0m19:32:56.684150 [error] [MainThread]:     
[0m19:32:56.684468 [error] [MainThread]:   
[0m19:32:56.684816 [info ] [MainThread]: 
[0m19:32:56.685187 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:32:56.685630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b8a2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129d005e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129b81e20>]}
[0m19:32:56.685952 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:35:52.678635 | df041f82-a71e-4d37-ae07-cf2d1cec5718 ==============================
[0m19:35:52.678635 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:35:52.680044 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_limit'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:35:52.680323 [debug] [MainThread]: Tracking: tracking
[0m19:35:52.690915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103946e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f40cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f402e0>]}
[0m19:35:52.721002 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:35:52.721298 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_limit.sql
[0m19:35:52.729828 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:35:52.737934 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:35:52.742882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df041f82-a71e-4d37-ae07-cf2d1cec5718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ae60d0>]}
[0m19:35:52.746938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df041f82-a71e-4d37-ae07-cf2d1cec5718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f9e8b0>]}
[0m19:35:52.747144 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:35:52.747359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df041f82-a71e-4d37-ae07-cf2d1cec5718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f9ec40>]}
[0m19:35:52.747973 [info ] [MainThread]: 
[0m19:35:52.748868 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:35:52.749374 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:35:52.754851 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:35:52.755103 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:35:52.755251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:35:53.714509 [debug] [ThreadPool]: SQL status: OK in 0.96 seconds
[0m19:35:53.732822 [debug] [ThreadPool]: On list_schemas: Close
[0m19:35:53.907602 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:35:53.922128 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:35:53.922534 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:35:53.922848 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:35:53.923152 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:35:54.750646 [debug] [ThreadPool]: SQL status: OK in 0.83 seconds
[0m19:35:54.761725 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:35:54.762030 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:35:55.315123 [debug] [ThreadPool]: SQL status: OK in 0.55 seconds
[0m19:35:55.323435 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:35:55.324059 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:35:55.324452 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:35:55.518431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df041f82-a71e-4d37-ae07-cf2d1cec5718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a00c40>]}
[0m19:35:55.519312 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:35:55.519767 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:35:55.520772 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:35:55.521438 [info ] [MainThread]: 
[0m19:35:55.527522 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:35:55.528124 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_limit ....................... [RUN]
[0m19:35:55.529196 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:35:55.529681 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:35:55.534032 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:35:55.535094 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:35:55.530002 => 2023-03-30 19:35:55.535003
[0m19:35:55.535487 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:35:55.582524 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_limit"
[0m19:35:55.582929 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:35:55.583098 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:35:55.583264 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
order by id
  
[0m19:35:55.583403 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:35:56.069475 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
order by id
  
[0m19:35:56.070758 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 23, pos 12)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in '[20, 2, 19]' 
------------^^^
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
order by id
  

[0m19:35:56.071588 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 23, pos 12)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in '[20, 2, 19]' 
------------^^^
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
order by id
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 23, pos 12)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in '[20, 2, 19]' 
------------^^^
-- 
    user_id in '[20, 2, 19]' 
-- 
    user_id in '[20, 2, 19]' 
-- 
order by id
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m19:35:56.072342 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf2\x17\x06\x10\xfd\x8b/\x10\x8d\x0c\xba\x03F'
[0m19:35:56.073803 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:35:55.535749 => 2023-03-30 19:35:56.073594
[0m19:35:56.074701 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: ROLLBACK
[0m19:35:56.075447 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:35:56.076102 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: Close
[0m19:35:56.286765 [debug] [Thread-1  ]: Runtime Error in model silver_cities_limit (models/silver_cities_limit.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 23, pos 12)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_limit`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select id, name, user_id from airbyte.bronze_cities
  where 
  -- 
      user_id in '[20, 2, 19]' 
  ------------^^^
  -- 
      user_id in '[20, 2, 19]' 
  -- 
      user_id in '[20, 2, 19]' 
  -- 
  order by id
    
  
[0m19:35:56.287112 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df041f82-a71e-4d37-ae07-cf2d1cec5718', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c9da60>]}
[0m19:35:56.287466 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_limit .............. [[31mERROR[0m in 0.76s]
[0m19:35:56.288492 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:35:56.289316 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:35:56.289529 [debug] [MainThread]: On master: ROLLBACK
[0m19:35:56.289709 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:35:56.478072 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:35:56.479669 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:35:56.480504 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:35:56.481375 [debug] [MainThread]: On master: ROLLBACK
[0m19:35:56.482178 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:35:56.482999 [debug] [MainThread]: On master: Close
[0m19:35:56.636559 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:35:56.637975 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:35:56.640406 [info ] [MainThread]: 
[0m19:35:56.641466 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.89 seconds (3.89s).
[0m19:35:56.642270 [debug] [MainThread]: Command end result
[0m19:35:56.655811 [info ] [MainThread]: 
[0m19:35:56.656516 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:35:56.657043 [info ] [MainThread]: 
[0m19:35:56.657572 [error] [MainThread]: [33mRuntime Error in model silver_cities_limit (models/silver_cities_limit.sql)[0m
[0m19:35:56.658070 [error] [MainThread]:   
[0m19:35:56.658557 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 23, pos 12)
[0m19:35:56.659019 [error] [MainThread]:   
[0m19:35:56.659405 [error] [MainThread]:   == SQL ==
[0m19:35:56.659790 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
[0m19:35:56.660175 [error] [MainThread]:   
[0m19:35:56.660557 [error] [MainThread]:     
[0m19:35:56.660937 [error] [MainThread]:       
[0m19:35:56.661319 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_limit`
[0m19:35:56.661699 [error] [MainThread]:         
[0m19:35:56.662081 [error] [MainThread]:         
[0m19:35:56.662459 [error] [MainThread]:       using delta
[0m19:35:56.662843 [error] [MainThread]:         
[0m19:35:56.663225 [error] [MainThread]:         
[0m19:35:56.663609 [error] [MainThread]:         
[0m19:35:56.664001 [error] [MainThread]:         
[0m19:35:56.664391 [error] [MainThread]:         
[0m19:35:56.664795 [error] [MainThread]:         
[0m19:35:56.665141 [error] [MainThread]:         as
[0m19:35:56.665527 [error] [MainThread]:         
[0m19:35:56.665905 [error] [MainThread]:   
[0m19:35:56.666266 [error] [MainThread]:   
[0m19:35:56.666613 [error] [MainThread]:   
[0m19:35:56.666955 [error] [MainThread]:   select id, name, user_id from airbyte.bronze_cities
[0m19:35:56.667286 [error] [MainThread]:   where 
[0m19:35:56.667608 [error] [MainThread]:   -- 
[0m19:35:56.667933 [error] [MainThread]:       user_id in '[20, 2, 19]' 
[0m19:35:56.668255 [error] [MainThread]:   ------------^^^
[0m19:35:56.668578 [error] [MainThread]:   -- 
[0m19:35:56.668901 [error] [MainThread]:       user_id in '[20, 2, 19]' 
[0m19:35:56.669226 [error] [MainThread]:   -- 
[0m19:35:56.669555 [error] [MainThread]:       user_id in '[20, 2, 19]' 
[0m19:35:56.669884 [error] [MainThread]:   -- 
[0m19:35:56.670223 [error] [MainThread]:   order by id
[0m19:35:56.670548 [error] [MainThread]:     
[0m19:35:56.670872 [error] [MainThread]:   
[0m19:35:56.671210 [info ] [MainThread]: 
[0m19:35:56.671567 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:35:56.671974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a89a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f9ebb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a00910>]}
[0m19:35:56.672276 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:37:35.569990 | 2ba5fd43-778a-4e1a-bcf5-1699ec1e2423 ==============================
[0m19:37:35.569990 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:37:35.571454 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_limit'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:37:35.571685 [debug] [MainThread]: Tracking: tracking
[0m19:37:35.582222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4ba430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10768eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4f1b20>]}
[0m19:37:35.614728 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:37:35.615041 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_limit.sql
[0m19:37:35.622809 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:37:35.631038 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:37:35.636057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2ba5fd43-778a-4e1a-bcf5-1699ec1e2423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9650d0>]}
[0m19:37:35.639950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2ba5fd43-778a-4e1a-bcf5-1699ec1e2423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e632820>]}
[0m19:37:35.640154 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:37:35.640372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2ba5fd43-778a-4e1a-bcf5-1699ec1e2423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10768eac0>]}
[0m19:37:35.640992 [info ] [MainThread]: 
[0m19:37:35.641870 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:37:35.642373 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:37:35.647782 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:37:35.648007 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:37:35.648142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:37:36.325066 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m19:37:36.341428 [debug] [ThreadPool]: On list_schemas: Close
[0m19:37:36.530131 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:37:36.543959 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:37:36.544286 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:37:36.544557 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:37:36.544809 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:37:37.196664 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m19:37:37.211865 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:37:37.212281 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:37:37.629448 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m19:37:37.637040 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:37:37.637696 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:37:37.638180 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:37:37.794890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2ba5fd43-778a-4e1a-bcf5-1699ec1e2423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e87f2e0>]}
[0m19:37:37.795767 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:37:37.796235 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:37:37.797167 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:37:37.797724 [info ] [MainThread]: 
[0m19:37:37.803724 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:37:37.804255 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_limit ....................... [RUN]
[0m19:37:37.805145 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:37:37.805480 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:37:37.809487 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:37:37.810345 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:37:37.805751 => 2023-03-30 19:37:37.810272
[0m19:37:37.810639 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:37:37.853784 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_limit"
[0m19:37:37.854468 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:37:37.854640 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:37:37.854804 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
-- 
order by id
  
[0m19:37:37.854941 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:37:38.326555 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
-- 
order by id
  
[0m19:37:38.326851 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
----^^^
-- 
    user_id in (20, 2, 19)
-- 
order by id
  

[0m19:37:38.327105 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
----^^^
-- 
    user_id in (20, 2, 19)
-- 
order by id
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
-- 
    user_id in (20, 2, 19)
-- 
    user_id in (20, 2, 19)
----^^^
-- 
    user_id in (20, 2, 19)
-- 
order by id
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m19:37:38.327337 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf2S\xfa\x17\x00\xae5\xc5\xb4\xc8\xd2\x8c/'
[0m19:37:38.327756 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:37:37.810851 => 2023-03-30 19:37:38.327702
[0m19:37:38.328015 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: ROLLBACK
[0m19:37:38.328233 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:37:38.328445 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: Close
[0m19:37:38.513961 [debug] [Thread-1  ]: Runtime Error in model silver_cities_limit (models/silver_cities_limit.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_limit`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select id, name, user_id from airbyte.bronze_cities
  where 
  -- 
      user_id in (20, 2, 19)
  -- 
      user_id in (20, 2, 19)
  ----^^^
  -- 
      user_id in (20, 2, 19)
  -- 
  order by id
    
  
[0m19:37:38.514279 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ba5fd43-778a-4e1a-bcf5-1699ec1e2423', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea51a30>]}
[0m19:37:38.514626 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_limit .............. [[31mERROR[0m in 0.71s]
[0m19:37:38.515708 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:37:38.516545 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:37:38.516752 [debug] [MainThread]: On master: ROLLBACK
[0m19:37:38.516928 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:37:38.699424 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:37:38.700923 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:37:38.701756 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:37:38.702604 [debug] [MainThread]: On master: ROLLBACK
[0m19:37:38.703036 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:37:38.703387 [debug] [MainThread]: On master: Close
[0m19:37:38.914081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:37:38.914455 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:37:38.915862 [info ] [MainThread]: 
[0m19:37:38.916424 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.27 seconds (3.27s).
[0m19:37:38.917086 [debug] [MainThread]: Command end result
[0m19:37:38.926663 [info ] [MainThread]: 
[0m19:37:38.927208 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:37:38.927622 [info ] [MainThread]: 
[0m19:37:38.928005 [error] [MainThread]: [33mRuntime Error in model silver_cities_limit (models/silver_cities_limit.sql)[0m
[0m19:37:38.928357 [error] [MainThread]:   
[0m19:37:38.928699 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'user_id'.(line 25, pos 4)
[0m19:37:38.929032 [error] [MainThread]:   
[0m19:37:38.929365 [error] [MainThread]:   == SQL ==
[0m19:37:38.929695 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */
[0m19:37:38.930033 [error] [MainThread]:   
[0m19:37:38.930358 [error] [MainThread]:     
[0m19:37:38.930686 [error] [MainThread]:       
[0m19:37:38.931013 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_limit`
[0m19:37:38.931348 [error] [MainThread]:         
[0m19:37:38.931678 [error] [MainThread]:         
[0m19:37:38.932004 [error] [MainThread]:       using delta
[0m19:37:38.932325 [error] [MainThread]:         
[0m19:37:38.932643 [error] [MainThread]:         
[0m19:37:38.932961 [error] [MainThread]:         
[0m19:37:38.933279 [error] [MainThread]:         
[0m19:37:38.933577 [error] [MainThread]:         
[0m19:37:38.933852 [error] [MainThread]:         
[0m19:37:38.934130 [error] [MainThread]:         as
[0m19:37:38.934405 [error] [MainThread]:         
[0m19:37:38.934681 [error] [MainThread]:   
[0m19:37:38.934958 [error] [MainThread]:   
[0m19:37:38.935231 [error] [MainThread]:   
[0m19:37:38.935504 [error] [MainThread]:   select id, name, user_id from airbyte.bronze_cities
[0m19:37:38.935779 [error] [MainThread]:   where 
[0m19:37:38.936051 [error] [MainThread]:   -- 
[0m19:37:38.936325 [error] [MainThread]:       user_id in (20, 2, 19)
[0m19:37:38.936599 [error] [MainThread]:   -- 
[0m19:37:38.936873 [error] [MainThread]:       user_id in (20, 2, 19)
[0m19:37:38.937150 [error] [MainThread]:   ----^^^
[0m19:37:38.937426 [error] [MainThread]:   -- 
[0m19:37:38.937703 [error] [MainThread]:       user_id in (20, 2, 19)
[0m19:37:38.937979 [error] [MainThread]:   -- 
[0m19:37:38.938255 [error] [MainThread]:   order by id
[0m19:37:38.938533 [error] [MainThread]:     
[0m19:37:38.938811 [error] [MainThread]:   
[0m19:37:38.939102 [info ] [MainThread]: 
[0m19:37:38.939430 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:37:38.939821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea7b3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e888160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e87fd30>]}
[0m19:37:38.940110 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:38:15.383262 | 5444dbac-5510-4838-9dcc-38a1c59256ea ==============================
[0m19:38:15.383262 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:38:15.384820 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_limit'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:38:15.385002 [debug] [MainThread]: Tracking: tracking
[0m19:38:15.393567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1b6b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10626eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1efb20>]}
[0m19:38:15.425281 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:38:15.425583 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_limit.sql
[0m19:38:15.433564 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:38:15.441689 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:38:15.446881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5444dbac-5510-4838-9dcc-38a1c59256ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5640d0>]}
[0m19:38:15.450800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5444dbac-5510-4838-9dcc-38a1c59256ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c231880>]}
[0m19:38:15.450996 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:38:15.451215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5444dbac-5510-4838-9dcc-38a1c59256ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10626eac0>]}
[0m19:38:15.451832 [info ] [MainThread]: 
[0m19:38:15.452735 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:38:15.453265 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:38:15.459467 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:38:15.459747 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:38:15.459891 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:16.121044 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
[0m19:38:16.135928 [debug] [ThreadPool]: On list_schemas: Close
[0m19:38:16.288481 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:38:16.306318 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:16.306753 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:38:16.307079 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:38:16.307384 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:16.932569 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m19:38:16.944102 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:38:16.944442 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:38:17.397746 [debug] [ThreadPool]: SQL status: OK in 0.45 seconds
[0m19:38:17.404934 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:38:17.405553 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:38:17.405987 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:38:17.624185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5444dbac-5510-4838-9dcc-38a1c59256ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c47f2e0>]}
[0m19:38:17.625118 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:17.625602 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:38:17.626624 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:38:17.627328 [info ] [MainThread]: 
[0m19:38:17.634515 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:38:17.635265 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_limit ....................... [RUN]
[0m19:38:17.636304 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:38:17.636756 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:38:17.640388 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:38:17.641335 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:38:17.637090 => 2023-03-30 19:38:17.641239
[0m19:38:17.641757 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:38:17.689224 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_limit"
[0m19:38:17.689635 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:17.689807 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:38:17.689982 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from airbyte.bronze_cities
where 
    user_id in (20, 2, 19)
order by id
  
[0m19:38:17.690125 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:38:29.199474 [debug] [Thread-1  ]: SQL status: OK in 11.51 seconds
[0m19:38:29.497877 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:38:17.642064 => 2023-03-30 19:38:29.497801
[0m19:38:29.498297 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: ROLLBACK
[0m19:38:29.498549 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:38:29.498758 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: Close
[0m19:38:29.641519 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5444dbac-5510-4838-9dcc-38a1c59256ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5ce820>]}
[0m19:38:29.642951 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities_limit .................. [[32mOK[0m in 12.01s]
[0m19:38:29.645815 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:38:29.648397 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:38:29.648926 [debug] [MainThread]: On master: ROLLBACK
[0m19:38:29.649316 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:38:29.934133 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:38:29.935523 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:29.936357 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:38:29.937240 [debug] [MainThread]: On master: ROLLBACK
[0m19:38:29.938059 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:38:29.938863 [debug] [MainThread]: On master: Close
[0m19:38:30.124361 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:38:30.125048 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:38:30.127134 [info ] [MainThread]: 
[0m19:38:30.127797 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 14.67 seconds (14.67s).
[0m19:38:30.128689 [debug] [MainThread]: Command end result
[0m19:38:30.138074 [info ] [MainThread]: 
[0m19:38:30.138618 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:38:30.139057 [info ] [MainThread]: 
[0m19:38:30.139485 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:38:30.139999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10626eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c67c3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c509ca0>]}
[0m19:38:30.140367 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:38:55.957850 | 75b3c919-0023-4c4b-b4d6-5547f68e9ffb ==============================
[0m19:38:55.957850 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:38:55.959330 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:38:55.959514 [debug] [MainThread]: Tracking: tracking
[0m19:38:55.968553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244b6b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105389ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244f1b20>]}
[0m19:38:55.999441 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:38:55.999641 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:38:56.003272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75b3c919-0023-4c4b-b4d6-5547f68e9ffb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1247bc0d0>]}
[0m19:38:56.007177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75b3c919-0023-4c4b-b4d6-5547f68e9ffb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1246cb280>]}
[0m19:38:56.007376 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:38:56.007589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75b3c919-0023-4c4b-b4d6-5547f68e9ffb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105389ac0>]}
[0m19:38:56.008227 [info ] [MainThread]: 
[0m19:38:56.009137 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:38:56.009645 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:38:56.015884 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:38:56.016168 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:38:56.016320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:56.593009 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m19:38:56.601188 [debug] [ThreadPool]: On list_schemas: Close
[0m19:38:56.749451 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:38:56.767247 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:56.767667 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:38:56.767983 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:38:56.768280 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:57.398056 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m19:38:57.411475 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:38:57.411838 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:38:57.890789 [debug] [ThreadPool]: SQL status: OK in 0.48 seconds
[0m19:38:57.897090 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:38:57.897542 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:38:57.897905 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:38:58.069094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75b3c919-0023-4c4b-b4d6-5547f68e9ffb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244b6eb0>]}
[0m19:38:58.069912 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:58.070366 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:38:58.071347 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:38:58.072063 [info ] [MainThread]: 
[0m19:38:58.077840 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m19:38:58.078335 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities ............................. [RUN]
[0m19:38:58.078908 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m19:38:58.079080 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m19:38:58.081647 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m19:38:58.082321 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-03-30 19:38:58.079215 => 2023-03-30 19:38:58.082229
[0m19:38:58.082716 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m19:38:58.100949 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:38:58.101219 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m19:38:58.101441 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

      describe extended `airbyte`.`silver_cities`
  
[0m19:38:58.101636 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:38:58.879232 [debug] [Thread-1  ]: SQL status: OK in 0.78 seconds
[0m19:38:58.926818 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m19:38:58.927331 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m19:38:58.927524 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `airbyte`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id
  
[0m19:39:02.774097 [debug] [Thread-1  ]: SQL status: OK in 3.85 seconds
[0m19:39:02.816112 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-03-30 19:38:58.083012 => 2023-03-30 19:39:02.816009
[0m19:39:02.816575 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m19:39:02.816903 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:39:02.817148 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m19:39:02.975022 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75b3c919-0023-4c4b-b4d6-5547f68e9ffb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249cecd0>]}
[0m19:39:02.977663 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities ........................ [[32mOK[0m in 4.90s]
[0m19:39:02.983471 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m19:39:02.986750 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:39:02.987465 [debug] [MainThread]: On master: ROLLBACK
[0m19:39:02.987991 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:39:03.179709 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:39:03.181413 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:39:03.182274 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:39:03.183165 [debug] [MainThread]: On master: ROLLBACK
[0m19:39:03.184010 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:39:03.184855 [debug] [MainThread]: On master: Close
[0m19:39:03.332597 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:39:03.334482 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m19:39:03.337509 [info ] [MainThread]: 
[0m19:39:03.338593 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.33 seconds (7.33s).
[0m19:39:03.339902 [debug] [MainThread]: Command end result
[0m19:39:03.353538 [info ] [MainThread]: 
[0m19:39:03.354273 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:39:03.354831 [info ] [MainThread]: 
[0m19:39:03.355367 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:39:03.356004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124963490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124963460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124631280>]}
[0m19:39:03.356467 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:39:37.907129 | b619a27a-53c6-4c48-b3ee-5a16da1eef0d ==============================
[0m19:39:37.907129 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:39:37.908696 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_limit'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:39:37.908924 [debug] [MainThread]: Tracking: tracking
[0m19:39:37.918668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bdc7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bdd25e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bdd28e0>]}
[0m19:39:37.951407 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:39:37.951726 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_limit.sql
[0m19:39:37.959800 [debug] [MainThread]: 1603: static parser failed on silver_cities_limit.sql
[0m19:39:37.968073 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_limit.sql
[0m19:39:37.973181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b619a27a-53c6-4c48-b3ee-5a16da1eef0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c0470d0>]}
[0m19:39:37.976963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b619a27a-53c6-4c48-b3ee-5a16da1eef0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be11790>]}
[0m19:39:37.977166 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:39:37.977381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b619a27a-53c6-4c48-b3ee-5a16da1eef0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be11d00>]}
[0m19:39:37.978014 [info ] [MainThread]: 
[0m19:39:37.978894 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:39:37.979383 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:39:37.984920 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:39:37.985175 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:39:37.985341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:39:38.657591 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m19:39:38.679783 [debug] [ThreadPool]: On list_schemas: Close
[0m19:39:38.905212 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:39:38.926933 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:39:38.927429 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:39:38.927822 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:39:38.928183 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:39:39.559035 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m19:39:39.578366 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:39:39.578868 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:39:39.965839 [debug] [ThreadPool]: SQL status: OK in 0.39 seconds
[0m19:39:39.972812 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:39:39.973502 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:39:39.973976 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:39:40.166869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b619a27a-53c6-4c48-b3ee-5a16da1eef0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf5fcd0>]}
[0m19:39:40.168534 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:39:40.169262 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:39:40.171069 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:39:40.172126 [info ] [MainThread]: 
[0m19:39:40.180329 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_limit
[0m19:39:40.181210 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_limit ....................... [RUN]
[0m19:39:40.183111 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_limit'
[0m19:39:40.183727 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_limit
[0m19:39:40.189226 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_limit"
[0m19:39:40.190744 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (compile): 2023-03-30 19:39:40.184271 => 2023-03-30 19:39:40.190591
[0m19:39:40.191272 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_limit
[0m19:39:40.211533 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:39:40.211847 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:39:40.212080 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

      describe extended `airbyte`.`silver_cities_limit`
  
[0m19:39:40.212279 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:39:40.834171 [debug] [Thread-1  ]: SQL status: OK in 0.62 seconds
[0m19:39:40.902575 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_limit"
[0m19:39:40.903938 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_limit"
[0m19:39:40.904162 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_limit"} */

  
    
        create or replace table `airbyte`.`silver_cities_limit`
      
      
    using delta
      
      
      
      
      
      
      as
      



select id, name, user_id from `airbyte`.`silver_cities`
where 
    user_id in (20, 2, 19)
order by id
  
[0m19:39:44.595891 [debug] [Thread-1  ]: SQL status: OK in 3.69 seconds
[0m19:39:44.636434 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_limit (execute): 2023-03-30 19:39:40.191611 => 2023-03-30 19:39:44.636354
[0m19:39:44.636929 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: ROLLBACK
[0m19:39:44.637240 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:39:44.637513 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_limit: Close
[0m19:39:44.783028 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b619a27a-53c6-4c48-b3ee-5a16da1eef0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c2b7ee0>]}
[0m19:39:44.785028 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities_limit .................. [[32mOK[0m in 4.60s]
[0m19:39:44.788958 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_limit
[0m19:39:44.792243 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:39:44.793165 [debug] [MainThread]: On master: ROLLBACK
[0m19:39:44.793909 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:39:45.005360 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:39:45.006999 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:39:45.007878 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:39:45.008772 [debug] [MainThread]: On master: ROLLBACK
[0m19:39:45.009595 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:39:45.010392 [debug] [MainThread]: On master: Close
[0m19:39:45.206863 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:39:45.208293 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_limit' was properly closed.
[0m19:39:45.211627 [info ] [MainThread]: 
[0m19:39:45.213078 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.23 seconds (7.23s).
[0m19:39:45.214818 [debug] [MainThread]: Command end result
[0m19:39:45.232766 [info ] [MainThread]: 
[0m19:39:45.233651 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:39:45.234342 [info ] [MainThread]: 
[0m19:39:45.234915 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:39:45.235554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be11d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf5fd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf5fe20>]}
[0m19:39:45.236048 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:54:13.158877 | f84a3754-fe43-4b82-b1a8-c72dc93a2690 ==============================
[0m19:54:13.158877 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:54:13.160249 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'parse_only': False, 'select': ['silver_cities_find'], 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m19:54:13.160472 [debug] [MainThread]: Tracking: tracking
[0m19:54:13.170579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b853160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b85d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b85deb0>]}
[0m19:54:13.200428 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m19:54:13.200718 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/silver_cities_find.sql
[0m19:54:13.207878 [debug] [MainThread]: 1603: static parser failed on silver_cities_find.sql
[0m19:54:13.216180 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_find.sql
[0m19:54:13.221191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f84a3754-fe43-4b82-b1a8-c72dc93a2690', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bbe60d0>]}
[0m19:54:13.225173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f84a3754-fe43-4b82-b1a8-c72dc93a2690', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb00e50>]}
[0m19:54:13.225373 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:54:13.225592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f84a3754-fe43-4b82-b1a8-c72dc93a2690', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b853160>]}
[0m19:54:13.226220 [info ] [MainThread]: 
[0m19:54:13.227095 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:54:13.227731 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:54:13.235888 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:54:13.236173 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:54:13.236384 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:54:13.236521 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:54:14.313555 [debug] [ThreadPool]: SQL status: OK in 1.08 seconds
[0m19:54:14.332040 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:54:14.332317 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:54:14.798256 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
[0m19:54:14.803609 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:54:14.804275 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:54:14.804748 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:54:15.084605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f84a3754-fe43-4b82-b1a8-c72dc93a2690', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba4a670>]}
[0m19:54:15.085893 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:54:15.086496 [info ] [MainThread]: 
[0m19:54:15.091753 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_find
[0m19:54:15.092569 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_find'
[0m19:54:15.092920 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_find
[0m19:54:15.096971 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_find"
[0m19:54:15.097597 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_find (compile): 2023-03-30 19:54:15.093199 => 2023-03-30 19:54:15.097530
[0m19:54:15.097892 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_find
[0m19:54:15.098150 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_find (execute): 2023-03-30 19:54:15.098104 => 2023-03-30 19:54:15.098119
[0m19:54:15.099892 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_find
[0m19:54:15.100667 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:54:15.100919 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_find' was properly closed.
[0m19:54:15.101258 [debug] [MainThread]: Command end result
[0m19:54:15.107231 [info ] [MainThread]: Done.
[0m19:54:15.107650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb00af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bbe6d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b853160>]}
[0m19:54:15.107934 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 19:54:52.060730 | 15245856-1c5c-40bd-a255-f230b42ca04c ==============================
[0m19:54:52.060730 [info ] [MainThread]: Running with dbt=1.4.1
[0m19:54:52.062024 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_find'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:54:52.062235 [debug] [MainThread]: Tracking: tracking
[0m19:54:52.071505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1422f2b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077d1ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14232cb20>]}
[0m19:54:52.095354 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:54:52.095533 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:54:52.098874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15245856-1c5c-40bd-a255-f230b42ca04c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1424fb0d0>]}
[0m19:54:52.102703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15245856-1c5c-40bd-a255-f230b42ca04c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1424226a0>]}
[0m19:54:52.102903 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m19:54:52.103121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15245856-1c5c-40bd-a255-f230b42ca04c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077d1ac0>]}
[0m19:54:52.103756 [info ] [MainThread]: 
[0m19:54:52.104654 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:54:52.105197 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m19:54:52.110829 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:54:52.111056 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:54:52.111201 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:54:52.879772 [debug] [ThreadPool]: SQL status: OK in 0.77 seconds
[0m19:54:52.889926 [debug] [ThreadPool]: On list_schemas: Close
[0m19:54:53.193904 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m19:54:53.208229 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:54:53.208656 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:54:53.208956 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m19:54:53.209219 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:54:54.123417 [debug] [ThreadPool]: SQL status: OK in 0.91 seconds
[0m19:54:54.134646 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m19:54:54.135024 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m19:54:54.650025 [debug] [ThreadPool]: SQL status: OK in 0.51 seconds
[0m19:54:54.656871 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m19:54:54.657512 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:54:54.657933 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m19:54:54.815414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15245856-1c5c-40bd-a255-f230b42ca04c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x142322f40>]}
[0m19:54:54.816746 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:54:54.817187 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:54:54.818201 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:54:54.818741 [info ] [MainThread]: 
[0m19:54:54.824046 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_find
[0m19:54:54.824728 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_find ........................ [RUN]
[0m19:54:54.825843 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_find'
[0m19:54:54.826198 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_find
[0m19:54:54.830807 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_find"
[0m19:54:54.831586 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_find (compile): 2023-03-30 19:54:54.826467 => 2023-03-30 19:54:54.831509
[0m19:54:54.831928 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_find
[0m19:54:54.876395 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_find"
[0m19:54:54.876803 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:54:54.876969 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_find"
[0m19:54:54.877127 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_find: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */

  
    
        create or replace table `airbyte`.`silver_cities_find`
      
      
    using delta
      
      
      
      
      
      
      as
      



select 

(case when name  = 'Regina' then amount end) as test

(case when name  = 'Brikama' then amount end) as test

(case when name  = 'Freetown' then amount end) as test

from `airbyte`.`silver_cities_limit`
  
[0m19:54:54.877269 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:54:55.572307 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */

  
    
        create or replace table `airbyte`.`silver_cities_find`
      
      
    using delta
      
      
      
      
      
      
      as
      



select 

(case when name  = 'Regina' then amount end) as test

(case when name  = 'Brikama' then amount end) as test

(case when name  = 'Freetown' then amount end) as test

from `airbyte`.`silver_cities_limit`
  
[0m19:54:55.573868 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 24, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */

  
    
        create or replace table `airbyte`.`silver_cities_find`
      
      
    using delta
      
      
      
      
      
      
      as
      



select 

(case when name  = 'Regina' then amount end) as test

(case when name  = 'Brikama' then amount end) as test
^^^

(case when name  = 'Freetown' then amount end) as test

from `airbyte`.`silver_cities_limit`
  

[0m19:54:55.574659 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 24, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */

  
    
        create or replace table `airbyte`.`silver_cities_find`
      
      
    using delta
      
      
      
      
      
      
      as
      



select 

(case when name  = 'Regina' then amount end) as test

(case when name  = 'Brikama' then amount end) as test
^^^

(case when name  = 'Freetown' then amount end) as test

from `airbyte`.`silver_cities_limit`
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 24, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */

  
    
        create or replace table `airbyte`.`silver_cities_find`
      
      
    using delta
      
      
      
      
      
      
      as
      



select 

(case when name  = 'Regina' then amount end) as test

(case when name  = 'Brikama' then amount end) as test
^^^

(case when name  = 'Freetown' then amount end) as test

from `airbyte`.`silver_cities_limit`
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m19:54:55.575394 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"\x01\xed\xcf4\xbe*\x1a}\xaac='\xd0\x7fL\x1a"
[0m19:54:55.576888 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_find (execute): 2023-03-30 19:54:54.832176 => 2023-03-30 19:54:55.576666
[0m19:54:55.577789 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_find: ROLLBACK
[0m19:54:55.578355 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:54:55.578591 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_find: Close
[0m19:54:55.865173 [debug] [Thread-1  ]: Runtime Error in model silver_cities_find (models/silver_cities_find.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 24, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_find`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select 
  
  (case when name  = 'Regina' then amount end) as test
  
  (case when name  = 'Brikama' then amount end) as test
  ^^^
  
  (case when name  = 'Freetown' then amount end) as test
  
  from `airbyte`.`silver_cities_limit`
    
  
[0m19:54:55.865545 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15245856-1c5c-40bd-a255-f230b42ca04c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1427353d0>]}
[0m19:54:55.865900 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_find ............... [[31mERROR[0m in 1.04s]
[0m19:54:55.867113 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_find
[0m19:54:55.868104 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:54:55.868316 [debug] [MainThread]: On master: ROLLBACK
[0m19:54:55.868500 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:54:56.051065 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:54:56.052046 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:54:56.052460 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:54:56.052914 [debug] [MainThread]: On master: ROLLBACK
[0m19:54:56.053296 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:54:56.053670 [debug] [MainThread]: On master: Close
[0m19:54:56.494231 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:54:56.495573 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_find' was properly closed.
[0m19:54:56.499177 [info ] [MainThread]: 
[0m19:54:56.500355 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.39 seconds (4.39s).
[0m19:54:56.501097 [debug] [MainThread]: Command end result
[0m19:54:56.510361 [info ] [MainThread]: 
[0m19:54:56.510844 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:54:56.511216 [info ] [MainThread]: 
[0m19:54:56.511591 [error] [MainThread]: [33mRuntime Error in model silver_cities_find (models/silver_cities_find.sql)[0m
[0m19:54:56.511950 [error] [MainThread]:   
[0m19:54:56.512291 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 24, pos 0)
[0m19:54:56.512632 [error] [MainThread]:   
[0m19:54:56.512963 [error] [MainThread]:   == SQL ==
[0m19:54:56.513294 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_find"} */
[0m19:54:56.513637 [error] [MainThread]:   
[0m19:54:56.513964 [error] [MainThread]:     
[0m19:54:56.514290 [error] [MainThread]:       
[0m19:54:56.514617 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_find`
[0m19:54:56.514941 [error] [MainThread]:         
[0m19:54:56.515270 [error] [MainThread]:         
[0m19:54:56.515595 [error] [MainThread]:       using delta
[0m19:54:56.515891 [error] [MainThread]:         
[0m19:54:56.516173 [error] [MainThread]:         
[0m19:54:56.516457 [error] [MainThread]:         
[0m19:54:56.516737 [error] [MainThread]:         
[0m19:54:56.517021 [error] [MainThread]:         
[0m19:54:56.517305 [error] [MainThread]:         
[0m19:54:56.517589 [error] [MainThread]:         as
[0m19:54:56.517866 [error] [MainThread]:         
[0m19:54:56.518144 [error] [MainThread]:   
[0m19:54:56.518424 [error] [MainThread]:   
[0m19:54:56.518704 [error] [MainThread]:   
[0m19:54:56.518980 [error] [MainThread]:   select 
[0m19:54:56.519263 [error] [MainThread]:   
[0m19:54:56.519541 [error] [MainThread]:   (case when name  = 'Regina' then amount end) as test
[0m19:54:56.519818 [error] [MainThread]:   
[0m19:54:56.520092 [error] [MainThread]:   (case when name  = 'Brikama' then amount end) as test
[0m19:54:56.520376 [error] [MainThread]:   ^^^
[0m19:54:56.520652 [error] [MainThread]:   
[0m19:54:56.520931 [error] [MainThread]:   (case when name  = 'Freetown' then amount end) as test
[0m19:54:56.521208 [error] [MainThread]:   
[0m19:54:56.521487 [error] [MainThread]:   from `airbyte`.`silver_cities_limit`
[0m19:54:56.521767 [error] [MainThread]:     
[0m19:54:56.522052 [error] [MainThread]:   
[0m19:54:56.522352 [info ] [MainThread]: 
[0m19:54:56.522671 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:54:56.523055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1423773d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x142322490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x142422070>]}
[0m19:54:56.523365 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:00:17.163601 | adb88b04-a4aa-42e0-96a3-ffca478bc04f ==============================
[0m20:00:17.163601 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:00:17.164839 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:00:17.165017 [debug] [MainThread]: Tracking: tracking
[0m20:00:17.172937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081b66a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1288c9790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1288c9eb0>]}
[0m20:00:17.201632 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m20:00:17.201911 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/silver_cities_case.sql
[0m20:00:17.202035 [debug] [MainThread]: Partial parsing: deleted file: dbx_cities://models/silver_cities_find.sql
[0m20:00:17.209412 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:00:17.217615 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:00:17.222859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adb88b04-a4aa-42e0-96a3-ffca478bc04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128d420d0>]}
[0m20:00:17.226581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adb88b04-a4aa-42e0-96a3-ffca478bc04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128a32d60>]}
[0m20:00:17.226776 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:00:17.226996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adb88b04-a4aa-42e0-96a3-ffca478bc04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128a32d90>]}
[0m20:00:17.227592 [info ] [MainThread]: 
[0m20:00:17.228438 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:00:17.228904 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:00:17.234368 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:00:17.234636 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:00:17.234817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:18.166415 [debug] [ThreadPool]: SQL status: OK in 0.93 seconds
[0m20:00:18.175588 [debug] [ThreadPool]: On list_schemas: Close
[0m20:00:18.336458 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:00:18.354606 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:18.355007 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:00:18.355328 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:00:18.355640 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:00:19.139739 [debug] [ThreadPool]: SQL status: OK in 0.78 seconds
[0m20:00:19.149188 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:00:19.149464 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:00:19.654309 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m20:00:19.660249 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:00:19.660762 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:00:19.661141 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:00:19.847895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adb88b04-a4aa-42e0-96a3-ffca478bc04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128c826d0>]}
[0m20:00:19.848957 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:19.849429 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:00:19.850578 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:00:19.851299 [info ] [MainThread]: 
[0m20:00:19.858793 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:00:19.859427 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:00:19.860477 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:00:19.860871 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:00:19.865617 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:00:19.866319 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:00:19.861184 => 2023-03-30 20:00:19.866240
[0m20:00:19.866652 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:00:19.912674 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:00:19.913089 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:19.913255 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:00:19.913420 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

(case when name  = 'Regina' then 'NewYork' end) as case_Regina,

(case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

(case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

from `airbyte`.`silver_cities_limit`
  
[0m20:00:19.913559 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:00:20.452972 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

(case when name  = 'Regina' then 'NewYork' end) as case_Regina,

(case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

(case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

from `airbyte`.`silver_cities_limit`
  
[0m20:00:20.453739 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'.(line 30, pos 14)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

(case when name  = 'Regina' then 'NewYork' end) as case_Regina,

(case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

(case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

from `airbyte`.`silver_cities_limit`
--------------^^^
  

[0m20:00:20.454183 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'.(line 30, pos 14)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

(case when name  = 'Regina' then 'NewYork' end) as case_Regina,

(case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

(case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

from `airbyte`.`silver_cities_limit`
--------------^^^
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'.(line 30, pos 14)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

(case when name  = 'Regina' then 'NewYork' end) as case_Regina,

(case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

(case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

from `airbyte`.`silver_cities_limit`
--------------^^^
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:00:20.454596 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf5\x7f\xdc\x14\xd1\xa2@a\xed\xdc}\x98\xa9'
[0m20:00:20.455408 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:00:19.866893 => 2023-03-30 20:00:20.455264
[0m20:00:20.455871 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:00:20.456233 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:00:20.456581 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:00:20.652804 [debug] [Thread-1  ]: Runtime Error in model silver_cities_case (models/silver_cities_case.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '.'.(line 30, pos 14)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_case`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select
  name,
  user_id
  
  (case when name  = 'Regina' then 'NewYork' end) as case_Regina,
  
  (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,
  
  (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,
  
  from `airbyte`.`silver_cities_limit`
  --------------^^^
    
  
[0m20:00:20.653261 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'adb88b04-a4aa-42e0-96a3-ffca478bc04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128f71280>]}
[0m20:00:20.653659 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_case ............... [[31mERROR[0m in 0.79s]
[0m20:00:20.654854 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:00:20.655819 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:00:20.656061 [debug] [MainThread]: On master: ROLLBACK
[0m20:00:20.656256 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:20.841088 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:00:20.842319 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:20.843153 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:00:20.843973 [debug] [MainThread]: On master: ROLLBACK
[0m20:00:20.844749 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:00:20.845550 [debug] [MainThread]: On master: Close
[0m20:00:20.994238 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:00:20.994962 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:00:20.997361 [info ] [MainThread]: 
[0m20:00:20.998194 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.77 seconds (3.77s).
[0m20:00:20.999221 [debug] [MainThread]: Command end result
[0m20:00:21.011668 [info ] [MainThread]: 
[0m20:00:21.012313 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:00:21.012761 [info ] [MainThread]: 
[0m20:00:21.013207 [error] [MainThread]: [33mRuntime Error in model silver_cities_case (models/silver_cities_case.sql)[0m
[0m20:00:21.013633 [error] [MainThread]:   
[0m20:00:21.014034 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '.'.(line 30, pos 14)
[0m20:00:21.014428 [error] [MainThread]:   
[0m20:00:21.014822 [error] [MainThread]:   == SQL ==
[0m20:00:21.015211 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */
[0m20:00:21.015600 [error] [MainThread]:   
[0m20:00:21.015927 [error] [MainThread]:     
[0m20:00:21.016249 [error] [MainThread]:       
[0m20:00:21.016577 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_case`
[0m20:00:21.016901 [error] [MainThread]:         
[0m20:00:21.017225 [error] [MainThread]:         
[0m20:00:21.017550 [error] [MainThread]:       using delta
[0m20:00:21.017872 [error] [MainThread]:         
[0m20:00:21.018192 [error] [MainThread]:         
[0m20:00:21.018512 [error] [MainThread]:         
[0m20:00:21.018835 [error] [MainThread]:         
[0m20:00:21.019154 [error] [MainThread]:         
[0m20:00:21.019474 [error] [MainThread]:         
[0m20:00:21.019795 [error] [MainThread]:         as
[0m20:00:21.020120 [error] [MainThread]:         
[0m20:00:21.020445 [error] [MainThread]:   
[0m20:00:21.020766 [error] [MainThread]:   
[0m20:00:21.021093 [error] [MainThread]:   
[0m20:00:21.021421 [error] [MainThread]:   select
[0m20:00:21.021759 [error] [MainThread]:   name,
[0m20:00:21.022099 [error] [MainThread]:   user_id
[0m20:00:21.022437 [error] [MainThread]:   
[0m20:00:21.022773 [error] [MainThread]:   (case when name  = 'Regina' then 'NewYork' end) as case_Regina,
[0m20:00:21.023106 [error] [MainThread]:   
[0m20:00:21.023435 [error] [MainThread]:   (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,
[0m20:00:21.023725 [error] [MainThread]:   
[0m20:00:21.024010 [error] [MainThread]:   (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,
[0m20:00:21.024293 [error] [MainThread]:   
[0m20:00:21.024573 [error] [MainThread]:   from `airbyte`.`silver_cities_limit`
[0m20:00:21.024854 [error] [MainThread]:   --------------^^^
[0m20:00:21.025132 [error] [MainThread]:     
[0m20:00:21.025412 [error] [MainThread]:   
[0m20:00:21.025711 [info ] [MainThread]: 
[0m20:00:21.026042 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:00:21.026394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128f78ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128f71280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128f66130>]}
[0m20:00:21.026656 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:00:39.435235 | 7298b840-4ad0-4bb4-b67a-811df74e85fb ==============================
[0m20:00:39.435235 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:00:39.436371 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:00:39.436544 [debug] [MainThread]: Tracking: tracking
[0m20:00:39.444140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126136b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e8eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12616fb20>]}
[0m20:00:39.467813 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:00:39.468103 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_case.sql
[0m20:00:39.475485 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:00:39.483334 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:00:39.488436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7298b840-4ad0-4bb4-b67a-811df74e85fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1263c20d0>]}
[0m20:00:39.492154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7298b840-4ad0-4bb4-b67a-811df74e85fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1261b5070>]}
[0m20:00:39.492351 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:00:39.492576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7298b840-4ad0-4bb4-b67a-811df74e85fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e8eac0>]}
[0m20:00:39.493201 [info ] [MainThread]: 
[0m20:00:39.494092 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:00:39.494655 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:00:39.500112 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:00:39.500348 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:00:39.500489 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:40.159439 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
[0m20:00:40.169767 [debug] [ThreadPool]: On list_schemas: Close
[0m20:00:40.324726 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:00:40.343610 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:40.344006 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:00:40.344340 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:00:40.344649 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:00:41.053893 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m20:00:41.064854 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:00:41.065219 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:00:41.566110 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m20:00:41.572241 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:00:41.572615 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:00:41.572895 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:00:41.727531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7298b840-4ad0-4bb4-b67a-811df74e85fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126301d60>]}
[0m20:00:41.728499 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:41.728886 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:00:41.730131 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:00:41.730929 [info ] [MainThread]: 
[0m20:00:41.735671 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:00:41.736306 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:00:41.737333 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:00:41.737670 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:00:41.742262 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:00:41.743004 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:00:41.737949 => 2023-03-30 20:00:41.742939
[0m20:00:41.743297 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:00:41.786938 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:00:41.787347 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:41.787507 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:00:41.787671 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown

from `airbyte`.`silver_cities_limit`
  
[0m20:00:41.787810 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:00:42.252201 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown

from `airbyte`.`silver_cities_limit`
  
[0m20:00:42.253324 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 26, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama
----^^^

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown

from `airbyte`.`silver_cities_limit`
  

[0m20:00:42.254114 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 26, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama
----^^^

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown

from `airbyte`.`silver_cities_limit`
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 26, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,
user_id

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama
----^^^

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown

from `airbyte`.`silver_cities_limit`
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:94)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:474)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:473)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:00:42.254875 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf5\x8c\xdb\x19\xa1\xbc\xbf\xc1d\x983/\x00'
[0m20:00:42.255519 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:00:41.743508 => 2023-03-30 20:00:42.255450
[0m20:00:42.255898 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:00:42.256228 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:00:42.256540 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:00:42.449548 [debug] [Thread-1  ]: Runtime Error in model silver_cities_case (models/silver_cities_case.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 26, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */
  
    
      
          create or replace table `airbyte`.`silver_cities_case`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  
  
  select
  name,
  user_id
  
      (case when name  = 'Regina' then 'NewYork' end) as case_Regina
  
      (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama
  ----^^^
  
      (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown
  
  from `airbyte`.`silver_cities_limit`
    
  
[0m20:00:42.449906 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7298b840-4ad0-4bb4-b67a-811df74e85fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264f9940>]}
[0m20:00:42.450295 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_case ............... [[31mERROR[0m in 0.71s]
[0m20:00:42.451531 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:00:42.452460 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:00:42.452675 [debug] [MainThread]: On master: ROLLBACK
[0m20:00:42.452865 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:42.683847 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:00:42.685317 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:00:42.685951 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:00:42.686447 [debug] [MainThread]: On master: ROLLBACK
[0m20:00:42.686868 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:00:42.687295 [debug] [MainThread]: On master: Close
[0m20:00:42.836169 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:00:42.837094 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:00:42.839437 [info ] [MainThread]: 
[0m20:00:42.840224 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.35 seconds (3.35s).
[0m20:00:42.841041 [debug] [MainThread]: Command end result
[0m20:00:42.853467 [info ] [MainThread]: 
[0m20:00:42.854087 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:00:42.854523 [info ] [MainThread]: 
[0m20:00:42.854970 [error] [MainThread]: [33mRuntime Error in model silver_cities_case (models/silver_cities_case.sql)[0m
[0m20:00:42.855394 [error] [MainThread]:   
[0m20:00:42.855804 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 26, pos 4)
[0m20:00:42.856215 [error] [MainThread]:   
[0m20:00:42.856616 [error] [MainThread]:   == SQL ==
[0m20:00:42.857007 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */
[0m20:00:42.857398 [error] [MainThread]:   
[0m20:00:42.857764 [error] [MainThread]:     
[0m20:00:42.858096 [error] [MainThread]:       
[0m20:00:42.858424 [error] [MainThread]:           create or replace table `airbyte`.`silver_cities_case`
[0m20:00:42.858752 [error] [MainThread]:         
[0m20:00:42.859082 [error] [MainThread]:         
[0m20:00:42.859409 [error] [MainThread]:       using delta
[0m20:00:42.859738 [error] [MainThread]:         
[0m20:00:42.860065 [error] [MainThread]:         
[0m20:00:42.860391 [error] [MainThread]:         
[0m20:00:42.860718 [error] [MainThread]:         
[0m20:00:42.861041 [error] [MainThread]:         
[0m20:00:42.861363 [error] [MainThread]:         
[0m20:00:42.861685 [error] [MainThread]:         as
[0m20:00:42.862007 [error] [MainThread]:         
[0m20:00:42.862328 [error] [MainThread]:   
[0m20:00:42.862649 [error] [MainThread]:   
[0m20:00:42.862977 [error] [MainThread]:   
[0m20:00:42.863303 [error] [MainThread]:   select
[0m20:00:42.863630 [error] [MainThread]:   name,
[0m20:00:42.863975 [error] [MainThread]:   user_id
[0m20:00:42.864306 [error] [MainThread]:   
[0m20:00:42.864633 [error] [MainThread]:       (case when name  = 'Regina' then 'NewYork' end) as case_Regina
[0m20:00:42.864959 [error] [MainThread]:   
[0m20:00:42.865284 [error] [MainThread]:       (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama
[0m20:00:42.865612 [error] [MainThread]:   ----^^^
[0m20:00:42.865895 [error] [MainThread]:   
[0m20:00:42.866175 [error] [MainThread]:       (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown
[0m20:00:42.866455 [error] [MainThread]:   
[0m20:00:42.866736 [error] [MainThread]:   from `airbyte`.`silver_cities_limit`
[0m20:00:42.867018 [error] [MainThread]:     
[0m20:00:42.867300 [error] [MainThread]:   
[0m20:00:42.867596 [info ] [MainThread]: 
[0m20:00:42.867914 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:00:42.868286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e8eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264f98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264f9940>]}
[0m20:00:42.868553 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:01:19.962396 | 04fc4aa5-14ab-4300-8517-0b2c7d38ba0a ==============================
[0m20:01:19.962396 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:01:19.963577 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:01:19.963751 [debug] [MainThread]: Tracking: tracking
[0m20:01:19.971274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c365790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c36d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c36d8e0>]}
[0m20:01:19.995382 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:01:19.995671 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_case.sql
[0m20:01:20.002985 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:01:20.010956 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:01:20.016114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '04fc4aa5-14ab-4300-8517-0b2c7d38ba0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c8c10d0>]}
[0m20:01:20.019957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '04fc4aa5-14ab-4300-8517-0b2c7d38ba0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3ad3d0>]}
[0m20:01:20.020151 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:01:20.020369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04fc4aa5-14ab-4300-8517-0b2c7d38ba0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3adeb0>]}
[0m20:01:20.020982 [info ] [MainThread]: 
[0m20:01:20.021874 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:20.022419 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:01:20.028011 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:01:20.028243 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:01:20.028386 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:20.784145 [debug] [ThreadPool]: SQL status: OK in 0.76 seconds
[0m20:01:20.796029 [debug] [ThreadPool]: On list_schemas: Close
[0m20:01:20.960111 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:01:20.974928 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:01:20.975320 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:01:20.975599 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:01:20.975863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:01:21.586794 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m20:01:21.597352 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:01:21.597709 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:01:22.118432 [debug] [ThreadPool]: SQL status: OK in 0.52 seconds
[0m20:01:22.122853 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:01:22.123263 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:01:22.123576 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:01:22.289783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '04fc4aa5-14ab-4300-8517-0b2c7d38ba0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c802dc0>]}
[0m20:01:22.290855 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:01:22.291275 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:01:22.292273 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:01:22.292924 [info ] [MainThread]: 
[0m20:01:22.298621 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:01:22.299279 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:01:22.300389 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:01:22.300787 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:01:22.305674 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:01:22.306432 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:01:22.301121 => 2023-03-30 20:01:22.306338
[0m20:01:22.306783 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:01:22.352139 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:01:22.352530 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:01:22.352692 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:01:22.352856 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina,

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:01:22.352990 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:01:23.008675 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as case_Regina,

    (case when name  = 'Brikama' then 'NewYork' end) as case_Brikama,

    (case when name  = 'Freetown' then 'NewYork' end) as case_Freetown,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:01:23.009227 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:01:23.009539 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_ROUTINE] org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:661)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2857)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:637)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:221)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.immutable.List.map(List.scala:305)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:221)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:362)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:294)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:426)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:291)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2833)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:01:23.009839 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf5\xa5\t\x1b0\xbf\xe3\x88\x08\xd9\nz\x0e'
[0m20:01:23.010468 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:01:22.307032 => 2023-03-30 20:01:23.010343
[0m20:01:23.010832 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:01:23.011132 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:01:23.011427 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:01:23.197377 [debug] [Thread-1  ]: Runtime Error in model silver_cities_case (models/silver_cities_case.sql)
  [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:01:23.197721 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '04fc4aa5-14ab-4300-8517-0b2c7d38ba0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9aac10>]}
[0m20:01:23.198090 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_case ............... [[31mERROR[0m in 0.90s]
[0m20:01:23.199199 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:01:23.200075 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:01:23.200294 [debug] [MainThread]: On master: ROLLBACK
[0m20:01:23.200484 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:23.370329 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:01:23.371766 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:01:23.372621 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:01:23.373503 [debug] [MainThread]: On master: ROLLBACK
[0m20:01:23.374313 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:01:23.375196 [debug] [MainThread]: On master: Close
[0m20:01:23.528606 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:01:23.529663 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:01:23.532534 [info ] [MainThread]: 
[0m20:01:23.533208 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.51 seconds (3.51s).
[0m20:01:23.533963 [debug] [MainThread]: Command end result
[0m20:01:23.543210 [info ] [MainThread]: 
[0m20:01:23.543660 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:01:23.544026 [info ] [MainThread]: 
[0m20:01:23.544390 [error] [MainThread]: [33mRuntime Error in model silver_cities_case (models/silver_cities_case.sql)[0m
[0m20:01:23.544742 [error] [MainThread]:   [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:01:23.545091 [info ] [MainThread]: 
[0m20:01:23.545441 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:01:23.545880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3ad460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9f02b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9e6070>]}
[0m20:01:23.546198 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:02:04.984068 | b50223dd-b304-4cec-8c15-0e1687a4505d ==============================
[0m20:02:04.984068 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:02:04.985221 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:02:04.985406 [debug] [MainThread]: Tracking: tracking
[0m20:02:04.992954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c26eb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c26ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c2a8b20>]}
[0m20:02:05.016544 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:02:05.016838 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_case.sql
[0m20:02:05.024221 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:02:05.031980 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:02:05.037051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b50223dd-b304-4cec-8c15-0e1687a4505d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c4f90d0>]}
[0m20:02:05.040838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b50223dd-b304-4cec-8c15-0e1687a4505d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c2e9070>]}
[0m20:02:05.041026 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:02:05.041263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b50223dd-b304-4cec-8c15-0e1687a4505d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c26ac0>]}
[0m20:02:05.041871 [info ] [MainThread]: 
[0m20:02:05.042763 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:02:05.043272 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:02:05.048711 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:02:05.048937 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:02:05.049076 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:05.857924 [debug] [ThreadPool]: SQL status: OK in 0.81 seconds
[0m20:02:05.865407 [debug] [ThreadPool]: On list_schemas: Close
[0m20:02:06.087315 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:02:06.100748 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:02:06.101085 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:02:06.101358 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:02:06.101610 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:02:06.943357 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
[0m20:02:06.959075 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:02:06.959554 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:02:07.398017 [debug] [ThreadPool]: SQL status: OK in 0.44 seconds
[0m20:02:07.407369 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:02:07.407870 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:02:07.408223 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:02:07.602779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b50223dd-b304-4cec-8c15-0e1687a4505d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c439d60>]}
[0m20:02:07.603387 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:02:07.603738 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:02:07.604579 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:02:07.605139 [info ] [MainThread]: 
[0m20:02:07.609589 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:02:07.610082 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:02:07.610911 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:02:07.611235 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:02:07.615624 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:02:07.616477 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:02:07.611495 => 2023-03-30 20:02:07.616402
[0m20:02:07.616762 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:02:07.659767 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:02:07.660170 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:02:07.660332 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:02:07.660497 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as Regina_case,

    (case when name  = 'Brikama' then 'NewYork' end) as Brikama_case,

    (case when name  = 'Freetown' then 'NewYork' end) as Freetown_case,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:02:07.660633 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:02:08.501760 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as Regina_case,

    (case when name  = 'Brikama' then 'NewYork' end) as Brikama_case,

    (case when name  = 'Freetown' then 'NewYork' end) as Freetown_case,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:02:08.502853 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:02:08.503637 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_ROUTINE] org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:661)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2857)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:637)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:221)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.immutable.List.map(List.scala:305)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:221)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:362)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:294)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:426)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:291)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2833)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:02:08.504404 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"\x01\xed\xcf5\xc0)\x14\xdf\x95\xb8\x82\xd6'\x88kP"
[0m20:02:08.505848 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:02:07.616969 => 2023-03-30 20:02:08.505645
[0m20:02:08.506708 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:02:08.507459 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:02:08.507925 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:02:08.725960 [debug] [Thread-1  ]: Runtime Error in model silver_cities_case (models/silver_cities_case.sql)
  [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:02:08.726325 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b50223dd-b304-4cec-8c15-0e1687a4505d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c630940>]}
[0m20:02:08.726741 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_case ............... [[31mERROR[0m in 1.12s]
[0m20:02:08.727933 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:02:08.728951 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:02:08.729177 [debug] [MainThread]: On master: ROLLBACK
[0m20:02:08.729364 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:02:09.058227 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:02:09.059312 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:02:09.059782 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:02:09.060292 [debug] [MainThread]: On master: ROLLBACK
[0m20:02:09.060750 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:02:09.061180 [debug] [MainThread]: On master: Close
[0m20:02:09.235824 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:02:09.236956 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:02:09.239290 [info ] [MainThread]: 
[0m20:02:09.240059 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.20 seconds (4.20s).
[0m20:02:09.240970 [debug] [MainThread]: Command end result
[0m20:02:09.254585 [info ] [MainThread]: 
[0m20:02:09.255189 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:02:09.255630 [info ] [MainThread]: 
[0m20:02:09.256070 [error] [MainThread]: [33mRuntime Error in model silver_cities_case (models/silver_cities_case.sql)[0m
[0m20:02:09.256486 [error] [MainThread]:   [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:02:09.256903 [info ] [MainThread]: 
[0m20:02:09.257326 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:02:09.257829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c26ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c6308b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c630940>]}
[0m20:02:09.258212 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:02:58.210633 | 52938ef1-69ab-401e-957c-165f5415dfab ==============================
[0m20:02:58.210633 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:02:58.211827 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:02:58.212047 [debug] [MainThread]: Tracking: tracking
[0m20:02:58.219554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125fefb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058aeac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126831b20>]}
[0m20:02:58.243323 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:02:58.243614 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_case.sql
[0m20:02:58.250887 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:02:58.258747 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:02:58.263717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52938ef1-69ab-401e-957c-165f5415dfab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126c820d0>]}
[0m20:02:58.267443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52938ef1-69ab-401e-957c-165f5415dfab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126876070>]}
[0m20:02:58.267638 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:02:58.267860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52938ef1-69ab-401e-957c-165f5415dfab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058aeac0>]}
[0m20:02:58.268454 [info ] [MainThread]: 
[0m20:02:58.269326 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:02:58.269827 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:02:58.275288 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:02:58.275519 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:02:58.275657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:59.300378 [debug] [ThreadPool]: SQL status: OK in 1.02 seconds
[0m20:02:59.310913 [debug] [ThreadPool]: On list_schemas: Close
[0m20:02:59.517396 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:02:59.534469 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:02:59.534942 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:02:59.535315 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:02:59.535617 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:03:00.387421 [debug] [ThreadPool]: SQL status: OK in 0.85 seconds
[0m20:03:00.403576 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:03:00.403987 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:03:00.872553 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
[0m20:03:00.877421 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:03:00.877869 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:03:00.878228 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:03:01.032464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52938ef1-69ab-401e-957c-165f5415dfab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bc3d60>]}
[0m20:03:01.033542 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:01.034004 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:03:01.035107 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:03:01.035809 [info ] [MainThread]: 
[0m20:03:01.041904 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:03:01.042584 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:03:01.043631 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:03:01.044016 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:03:01.049014 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:03:01.049737 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:03:01.044336 => 2023-03-30 20:03:01.049658
[0m20:03:01.050073 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:03:01.096673 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:03:01.097119 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:01.097287 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:03:01.097460 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as Regina_case,

    (case when name  = 'Brikama' then 'NewYork' end) as Brikama_case,

    (case when name  = 'Freetown' then 'NewYork' end) as Freetown_case,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:03:01.097609 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:01.820691 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name

    (case when name  = 'Regina' then 'NewYork' end) as Regina_case,

    (case when name  = 'Brikama' then 'NewYork' end) as Brikama_case,

    (case when name  = 'Freetown' then 'NewYork' end) as Freetown_case,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:03:01.821351 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:03:01.821761 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_ROUTINE] org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:661)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2857)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:637)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:524)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:205)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:221)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.immutable.List.map(List.scala:305)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:221)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:362)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:226)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:164)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:294)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:426)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:293)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:291)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2839)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2833)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:03:01.822131 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf5\xdf\xf7\x13\t\xbf\x02\x91\xdf\x05z\xd3\xc2'
[0m20:03:01.822893 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:03:01.050316 => 2023-03-30 20:03:01.822751
[0m20:03:01.823343 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:03:01.823706 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:03:01.824055 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:03:02.224363 [debug] [Thread-1  ]: Runtime Error in model silver_cities_case (models/silver_cities_case.sql)
  [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:03:02.224679 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52938ef1-69ab-401e-957c-165f5415dfab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126db9940>]}
[0m20:03:02.225031 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_cities_case ............... [[31mERROR[0m in 1.18s]
[0m20:03:02.226224 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:03:02.227235 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:03:02.227471 [debug] [MainThread]: On master: ROLLBACK
[0m20:03:02.227670 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:03:02.555909 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:03:02.557249 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:02.557614 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:03:02.558036 [debug] [MainThread]: On master: ROLLBACK
[0m20:03:02.558395 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:03:02.558814 [debug] [MainThread]: On master: Close
[0m20:03:02.838109 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:03:02.839261 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:03:02.841922 [info ] [MainThread]: 
[0m20:03:02.842664 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.57 seconds (4.57s).
[0m20:03:02.843463 [debug] [MainThread]: Command end result
[0m20:03:02.855763 [info ] [MainThread]: 
[0m20:03:02.856341 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:03:02.856785 [info ] [MainThread]: 
[0m20:03:02.857220 [error] [MainThread]: [33mRuntime Error in model silver_cities_case (models/silver_cities_case.sql)[0m
[0m20:03:02.857639 [error] [MainThread]:   [UNRESOLVED_ROUTINE] Cannot resolve function `name` on search path [`system`.`builtin`, `system`.`session`, `hive_metastore`.`default`].; line 21 pos 0
[0m20:03:02.858050 [info ] [MainThread]: 
[0m20:03:02.858470 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:03:02.858971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058aeac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126db97f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126db98b0>]}
[0m20:03:02.859344 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:03:27.235272 | 298cb7a6-bc51-40f6-84cd-ee69ff56bafe ==============================
[0m20:03:27.235272 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:03:27.236445 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities_case'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:03:27.236636 [debug] [MainThread]: Tracking: tracking
[0m20:03:27.244206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c96a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13086f790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13086feb0>]}
[0m20:03:27.268245 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:03:27.268544 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities_case.sql
[0m20:03:27.275900 [debug] [MainThread]: 1603: static parser failed on silver_cities_case.sql
[0m20:03:27.283740 [debug] [MainThread]: 1602: parser fallback to jinja rendering on silver_cities_case.sql
[0m20:03:27.289532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '298cb7a6-bc51-40f6-84cd-ee69ff56bafe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ac00d0>]}
[0m20:03:27.293354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '298cb7a6-bc51-40f6-84cd-ee69ff56bafe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1308b1d60>]}
[0m20:03:27.293549 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:03:27.293764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '298cb7a6-bc51-40f6-84cd-ee69ff56bafe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1308b1d90>]}
[0m20:03:27.294366 [info ] [MainThread]: 
[0m20:03:27.295210 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:03:27.295729 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:03:27.301133 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:03:27.301360 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:03:27.301504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:03:28.011003 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m20:03:28.020367 [debug] [ThreadPool]: On list_schemas: Close
[0m20:03:28.302995 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:03:28.321599 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:28.322058 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:03:28.322380 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:03:28.322691 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:03:29.179771 [debug] [ThreadPool]: SQL status: OK in 0.86 seconds
[0m20:03:29.191830 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:03:29.192414 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:03:29.706833 [debug] [ThreadPool]: SQL status: OK in 0.51 seconds
[0m20:03:29.713915 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:03:29.714498 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:03:29.715112 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:03:29.891237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '298cb7a6-bc51-40f6-84cd-ee69ff56bafe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130a026d0>]}
[0m20:03:29.892495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:29.892915 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:03:29.893868 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:03:29.894599 [info ] [MainThread]: 
[0m20:03:29.899614 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities_case
[0m20:03:29.900272 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities_case ........................ [RUN]
[0m20:03:29.901381 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities_case'
[0m20:03:29.901782 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities_case
[0m20:03:29.906533 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities_case"
[0m20:03:29.907308 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (compile): 2023-03-30 20:03:29.902097 => 2023-03-30 20:03:29.907231
[0m20:03:29.907652 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities_case
[0m20:03:29.951747 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities_case"
[0m20:03:29.952151 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:29.952314 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities_case"
[0m20:03:29.952473 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities_case"} */

  
    
        create or replace table `airbyte`.`silver_cities_case`
      
      
    using delta
      
      
      
      
      
      
      as
      



select
name,

    (case when name  = 'Regina' then 'NewYork' end) as Regina_case,

    (case when name  = 'Brikama' then 'NewYork' end) as Brikama_case,

    (case when name  = 'Freetown' then 'NewYork' end) as Freetown_case,

user_id
from `airbyte`.`silver_cities_limit`
  
[0m20:03:29.952607 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:03:33.557663 [debug] [Thread-1  ]: SQL status: OK in 3.6 seconds
[0m20:03:33.585357 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities_case (execute): 2023-03-30 20:03:29.907896 => 2023-03-30 20:03:33.585293
[0m20:03:33.585694 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: ROLLBACK
[0m20:03:33.585912 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:03:33.586115 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities_case: Close
[0m20:03:33.738879 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '298cb7a6-bc51-40f6-84cd-ee69ff56bafe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130c2a550>]}
[0m20:03:33.740586 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities_case ................... [[32mOK[0m in 3.84s]
[0m20:03:33.743720 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities_case
[0m20:03:33.746643 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:03:33.747296 [debug] [MainThread]: On master: ROLLBACK
[0m20:03:33.747769 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:03:34.089643 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:03:34.091108 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:03:34.091982 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:03:34.092856 [debug] [MainThread]: On master: ROLLBACK
[0m20:03:34.093690 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:03:34.094492 [debug] [MainThread]: On master: Close
[0m20:03:34.422987 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:03:34.424110 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities_case' was properly closed.
[0m20:03:34.427639 [info ] [MainThread]: 
[0m20:03:34.428777 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.13 seconds (7.13s).
[0m20:03:34.429523 [debug] [MainThread]: Command end result
[0m20:03:34.438853 [info ] [MainThread]: 
[0m20:03:34.439387 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:03:34.439820 [info ] [MainThread]: 
[0m20:03:34.440246 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:03:34.440693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130bf02e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130b72f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130a026d0>]}
[0m20:03:34.441019 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:15:04.778857 | ea67edaf-10a1-4aad-9edc-3263a904c327 ==============================
[0m20:15:04.778857 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:15:04.780172 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:15:04.780737 [debug] [MainThread]: Tracking: tracking
[0m20:15:04.789577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa28220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa315e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa31eb0>]}
[0m20:15:04.819721 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:15:04.820028 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m20:15:04.827581 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m20:15:04.838984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea67edaf-10a1-4aad-9edc-3263a904c327', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ac920d0>]}
[0m20:15:04.842762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea67edaf-10a1-4aad-9edc-3263a904c327', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa71af0>]}
[0m20:15:04.842955 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:15:04.843168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea67edaf-10a1-4aad-9edc-3263a904c327', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aa28220>]}
[0m20:15:04.843794 [info ] [MainThread]: 
[0m20:15:04.844665 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:04.845150 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:15:04.850830 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:15:04.851153 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:15:04.851332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:05.629148 [debug] [ThreadPool]: SQL status: OK in 0.78 seconds
[0m20:15:05.645861 [debug] [ThreadPool]: On list_schemas: Close
[0m20:15:05.800335 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:15:05.813131 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:05.813435 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:15:05.813696 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:15:05.813948 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:15:06.641489 [debug] [ThreadPool]: SQL status: OK in 0.83 seconds
[0m20:15:06.655895 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:15:06.656340 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:15:07.158310 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m20:15:07.162389 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:15:07.162779 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:15:07.163088 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:15:07.313319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea67edaf-10a1-4aad-9edc-3263a904c327', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abc3d60>]}
[0m20:15:07.313955 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:07.314309 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:15:07.315081 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:15:07.315538 [info ] [MainThread]: 
[0m20:15:07.320992 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m20:15:07.321486 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m20:15:07.322331 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m20:15:07.322618 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m20:15:07.325671 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m20:15:07.326444 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-03-30 20:15:07.322855 => 2023-03-30 20:15:07.326375
[0m20:15:07.326734 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m20:15:07.342308 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:07.342587 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m20:15:07.342792 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m20:15:07.342963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:15:08.182791 [debug] [Thread-1  ]: SQL status: OK in 0.84 seconds
[0m20:15:08.219664 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m20:15:08.220699 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m20:15:08.220885 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from `airbyte`.`silver_cities_case` cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:15:08.739141 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from `airbyte`.`silver_cities_case` cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:15:08.740214 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:08.741021 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [COLUMN_ALREADY_EXISTS] org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:2511)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:113)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:343)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:15:08.741467 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf7\x910\x10M\x85\xbb\xea\xbev.\xf3\xf6'
[0m20:15:08.742365 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-03-30 20:15:07.326943 => 2023-03-30 20:15:08.742196
[0m20:15:08.742879 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m20:15:08.743284 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:15:08.743644 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m20:15:08.939209 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:08.939645 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea67edaf-10a1-4aad-9edc-3263a904c327', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adb9a30>]}
[0m20:15:08.940047 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.62s]
[0m20:15:08.941364 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m20:15:08.942571 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:08.942843 [debug] [MainThread]: On master: ROLLBACK
[0m20:15:08.943047 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:15:09.127736 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:15:09.128970 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:09.129559 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:15:09.130064 [debug] [MainThread]: On master: ROLLBACK
[0m20:15:09.130568 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:15:09.131018 [debug] [MainThread]: On master: Close
[0m20:15:09.305429 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:15:09.306791 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m20:15:09.310088 [info ] [MainThread]: 
[0m20:15:09.310803 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.47 seconds (4.47s).
[0m20:15:09.311512 [debug] [MainThread]: Command end result
[0m20:15:09.321630 [info ] [MainThread]: 
[0m20:15:09.322168 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:15:09.322635 [info ] [MainThread]: 
[0m20:15:09.323063 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m20:15:09.323469 [error] [MainThread]:   [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:09.323828 [info ] [MainThread]: 
[0m20:15:09.324206 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:15:09.324651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11acb20d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abc3340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abc3dc0>]}
[0m20:15:09.324983 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:15:52.355555 | 8cd6a2ff-fe98-4463-b7a3-37d7fa19e923 ==============================
[0m20:15:52.355555 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:15:52.357021 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:15:52.357293 [debug] [MainThread]: Tracking: tracking
[0m20:15:52.366104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c34cf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c371220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c371640>]}
[0m20:15:52.394902 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m20:15:52.395173 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/silver_users_cities_join.sql
[0m20:15:52.395298 [debug] [MainThread]: Partial parsing: deleted file: dbx_cities://models/gold_users_cities_join.sql
[0m20:15:52.402656 [debug] [MainThread]: 1699: static parser successfully parsed silver_users_cities_join.sql
[0m20:15:52.414829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8cd6a2ff-fe98-4463-b7a3-37d7fa19e923', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c5d30d0>]}
[0m20:15:52.418864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8cd6a2ff-fe98-4463-b7a3-37d7fa19e923', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c3bb850>]}
[0m20:15:52.419086 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:15:52.419304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cd6a2ff-fe98-4463-b7a3-37d7fa19e923', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c3bb0a0>]}
[0m20:15:52.419970 [info ] [MainThread]: 
[0m20:15:52.420879 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:52.421402 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:15:52.427405 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:15:52.427681 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:15:52.427840 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:53.039999 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m20:15:53.055336 [debug] [ThreadPool]: On list_schemas: Close
[0m20:15:53.209855 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:15:53.227558 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:53.227951 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:15:53.228274 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:15:53.228570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:15:53.800364 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
[0m20:15:53.813550 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:15:53.813951 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:15:54.259004 [debug] [ThreadPool]: SQL status: OK in 0.44 seconds
[0m20:15:54.264049 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:15:54.264532 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:15:54.264895 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:15:54.435600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cd6a2ff-fe98-4463-b7a3-37d7fa19e923', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c501eb0>]}
[0m20:15:54.436583 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:54.437114 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:15:54.438335 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:15:54.439026 [info ] [MainThread]: 
[0m20:15:54.445832 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_users_cities_join
[0m20:15:54.446345 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_users_cities_join .................. [RUN]
[0m20:15:54.447224 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_users_cities_join'
[0m20:15:54.447555 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_users_cities_join
[0m20:15:54.451108 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_users_cities_join"
[0m20:15:54.451763 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (compile): 2023-03-30 20:15:54.447822 => 2023-03-30 20:15:54.451691
[0m20:15:54.452059 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_users_cities_join
[0m20:15:54.495575 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_users_cities_join"
[0m20:15:54.495992 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:54.496156 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_users_cities_join"
[0m20:15:54.496311 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_users_cities_join"} */

  
    
        create or replace table `airbyte`.`silver_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from `airbyte`.`silver_cities_case` cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:15:54.496445 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:15:55.312511 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_users_cities_join"} */

  
    
        create or replace table `airbyte`.`silver_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from `airbyte`.`silver_cities_case` cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:15:55.314064 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:55.314855 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [COLUMN_ALREADY_EXISTS] org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:2511)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:113)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:343)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:15:55.315622 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf7\xac\xe2\x19\xfb\x82\x99`x\xcbf\xd2Z'
[0m20:15:55.317169 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (execute): 2023-03-30 20:15:54.452267 => 2023-03-30 20:15:55.316918
[0m20:15:55.318102 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: ROLLBACK
[0m20:15:55.318852 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:15:55.319655 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: Close
[0m20:15:55.521183 [debug] [Thread-1  ]: Runtime Error in model silver_users_cities_join (models/silver_users_cities_join.sql)
  [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:55.521545 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cd6a2ff-fe98-4463-b7a3-37d7fa19e923', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e331e20>]}
[0m20:15:55.521915 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_users_cities_join ......... [[31mERROR[0m in 1.07s]
[0m20:15:55.523205 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_users_cities_join
[0m20:15:55.524277 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:15:55.524505 [debug] [MainThread]: On master: ROLLBACK
[0m20:15:55.524697 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:15:55.690446 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:15:55.691921 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:15:55.692698 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:15:55.693217 [debug] [MainThread]: On master: ROLLBACK
[0m20:15:55.693668 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:15:55.694096 [debug] [MainThread]: On master: Close
[0m20:15:55.841479 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:15:55.842367 [debug] [MainThread]: Connection 'model.dbx_cities.silver_users_cities_join' was properly closed.
[0m20:15:55.843993 [info ] [MainThread]: 
[0m20:15:55.844683 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.42 seconds (3.42s).
[0m20:15:55.845387 [debug] [MainThread]: Command end result
[0m20:15:55.857011 [info ] [MainThread]: 
[0m20:15:55.857572 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:15:55.858005 [info ] [MainThread]: 
[0m20:15:55.858437 [error] [MainThread]: [33mRuntime Error in model silver_users_cities_join (models/silver_users_cities_join.sql)[0m
[0m20:15:55.858886 [error] [MainThread]:   [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:15:55.859292 [info ] [MainThread]: 
[0m20:15:55.859717 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:15:55.860228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c3bb670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c3bc400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e25d5e0>]}
[0m20:15:55.860610 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:20:28.284515 | ceea2262-052f-4eb8-b783-9634ba126336 ==============================
[0m20:20:28.284515 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:20:28.285938 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:20:28.286179 [debug] [MainThread]: Tracking: tracking
[0m20:20:28.296783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121739430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103872ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121772b20>]}
[0m20:20:28.327433 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:20:28.327741 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_users_cities_join.sql
[0m20:20:28.335242 [debug] [MainThread]: 1699: static parser successfully parsed silver_users_cities_join.sql
[0m20:20:28.346558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ceea2262-052f-4eb8-b783-9634ba126336', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1219d20d0>]}
[0m20:20:28.350332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ceea2262-052f-4eb8-b783-9634ba126336', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217b5070>]}
[0m20:20:28.350530 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:20:28.350780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ceea2262-052f-4eb8-b783-9634ba126336', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103872ac0>]}
[0m20:20:28.351394 [info ] [MainThread]: 
[0m20:20:28.352277 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:20:28.352773 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:20:28.358912 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:20:28.359191 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:20:28.359345 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:20:29.291576 [debug] [ThreadPool]: SQL status: OK in 0.93 seconds
[0m20:20:29.310152 [debug] [ThreadPool]: On list_schemas: Close
[0m20:20:29.552375 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:20:29.570555 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:20:29.570934 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:20:29.571250 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:20:29.571547 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:20:30.230094 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
[0m20:20:30.246622 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:20:30.248062 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:20:30.737051 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m20:20:30.741606 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:20:30.742086 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:20:30.742400 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:20:30.921069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ceea2262-052f-4eb8-b783-9634ba126336', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121903d60>]}
[0m20:20:30.921865 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:20:30.922262 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:20:30.923140 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:20:30.923734 [info ] [MainThread]: 
[0m20:20:30.930718 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_users_cities_join
[0m20:20:30.931367 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_users_cities_join .................. [RUN]
[0m20:20:30.932453 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_users_cities_join'
[0m20:20:30.932875 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_users_cities_join
[0m20:20:30.937243 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_users_cities_join"
[0m20:20:30.938531 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (compile): 2023-03-30 20:20:30.933197 => 2023-03-30 20:20:30.938427
[0m20:20:30.938881 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_users_cities_join
[0m20:20:30.985593 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_users_cities_join"
[0m20:20:30.986033 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:20:30.986205 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_users_cities_join"
[0m20:20:30.986376 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_users_cities_join"} */

  
    
        create or replace table `airbyte`.`silver_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from airbyte.silver_cities_case cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:20:30.986518 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:20:31.790629 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_users_cities_join"} */

  
    
        create or replace table `airbyte`.`silver_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from airbyte.silver_cities_case cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:20:31.791785 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:20:31.792571 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [COLUMN_ALREADY_EXISTS] org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:597)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:496)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:360)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:338)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:323)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:372)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:2511)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:113)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:343)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:361)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:261)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:282)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:334)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:333)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:153)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:372)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:808)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:372)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:147)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:488)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1020)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:457)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:471)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:531)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:708)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:61)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:531)
	... 20 more

[0m20:20:31.793241 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xcf8Q\xaf\x10D\x83\xdas\xfa2MhM'
[0m20:20:31.793931 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (execute): 2023-03-30 20:20:30.939134 => 2023-03-30 20:20:31.793796
[0m20:20:31.794360 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: ROLLBACK
[0m20:20:31.794683 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:20:31.795000 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: Close
[0m20:20:32.027998 [debug] [Thread-1  ]: Runtime Error in model silver_users_cities_join (models/silver_users_cities_join.sql)
  [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:20:32.028331 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ceea2262-052f-4eb8-b783-9634ba126336', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121b2afa0>]}
[0m20:20:32.028686 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.silver_users_cities_join ......... [[31mERROR[0m in 1.10s]
[0m20:20:32.029910 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_users_cities_join
[0m20:20:32.030936 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:20:32.031165 [debug] [MainThread]: On master: ROLLBACK
[0m20:20:32.031358 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:20:32.205546 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:20:32.206673 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:20:32.207215 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:20:32.207739 [debug] [MainThread]: On master: ROLLBACK
[0m20:20:32.208199 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:20:32.208633 [debug] [MainThread]: On master: Close
[0m20:20:32.445330 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:20:32.446784 [debug] [MainThread]: Connection 'model.dbx_cities.silver_users_cities_join' was properly closed.
[0m20:20:32.450059 [info ] [MainThread]: 
[0m20:20:32.451076 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.10 seconds (4.10s).
[0m20:20:32.451682 [debug] [MainThread]: Command end result
[0m20:20:32.460496 [info ] [MainThread]: 
[0m20:20:32.460945 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:20:32.461325 [info ] [MainThread]: 
[0m20:20:32.461696 [error] [MainThread]: [33mRuntime Error in model silver_users_cities_join (models/silver_users_cities_join.sql)[0m
[0m20:20:32.462070 [error] [MainThread]:   [COLUMN_ALREADY_EXISTS] The column `user_id` already exists. Consider to choose another name or rename the existing column.
[0m20:20:32.462421 [info ] [MainThread]: 
[0m20:20:32.462746 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:20:32.463158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103872ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121903640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121903d30>]}
[0m20:20:32.463469 [debug] [MainThread]: Flushing usage events


============================== 2023-03-30 20:25:20.643804 | ba34f9e6-b695-4d90-836f-a3253696df2a ==============================
[0m20:25:20.643804 [info ] [MainThread]: Running with dbt=1.4.1
[0m20:25:20.645675 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:25:20.645890 [debug] [MainThread]: Tracking: tracking
[0m20:25:20.654776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e3e20a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e3ec5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e3eceb0>]}
[0m20:25:20.683414 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:25:20.683716 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_users_cities_join.sql
[0m20:25:20.692578 [debug] [MainThread]: 1699: static parser successfully parsed silver_users_cities_join.sql
[0m20:25:20.704062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ba34f9e6-b695-4d90-836f-a3253696df2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e7520d0>]}
[0m20:25:20.708065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ba34f9e6-b695-4d90-836f-a3253696df2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e42d670>]}
[0m20:25:20.708284 [info ] [MainThread]: Found 7 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m20:25:20.708513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba34f9e6-b695-4d90-836f-a3253696df2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e3e20a0>]}
[0m20:25:20.709279 [info ] [MainThread]: 
[0m20:25:20.710185 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:25:20.710725 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m20:25:20.716943 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m20:25:20.717278 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:25:20.717479 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:25:21.609163 [debug] [ThreadPool]: SQL status: OK in 0.89 seconds
[0m20:25:21.624253 [debug] [ThreadPool]: On list_schemas: Close
[0m20:25:21.816691 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m20:25:21.834480 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:25:21.834887 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:25:21.835208 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m20:25:21.835505 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:25:22.534766 [debug] [ThreadPool]: SQL status: OK in 0.7 seconds
[0m20:25:22.548554 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m20:25:22.548944 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m20:25:23.093337 [debug] [ThreadPool]: SQL status: OK in 0.54 seconds
[0m20:25:23.098059 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m20:25:23.098524 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:25:23.098885 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m20:25:23.379592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba34f9e6-b695-4d90-836f-a3253696df2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e681d60>]}
[0m20:25:23.380216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:25:23.380581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:25:23.381446 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:25:23.382017 [info ] [MainThread]: 
[0m20:25:23.387899 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_users_cities_join
[0m20:25:23.388396 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_users_cities_join .................. [RUN]
[0m20:25:23.389240 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_users_cities_join'
[0m20:25:23.389560 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_users_cities_join
[0m20:25:23.392586 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_users_cities_join"
[0m20:25:23.393422 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (compile): 2023-03-30 20:25:23.389818 => 2023-03-30 20:25:23.393316
[0m20:25:23.393744 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_users_cities_join
[0m20:25:23.436534 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_users_cities_join"
[0m20:25:23.437114 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:25:23.437274 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_users_cities_join"
[0m20:25:23.437432 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_users_cities_join"} */

  
    
        create or replace table `airbyte`.`silver_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select cc.name, cc.user_id, bu.user_name from airbyte.silver_cities_case cc
join airbyte.bronze_users bu on bu.user_id == cc.user_id
  
[0m20:25:23.437577 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:25:27.498232 [debug] [Thread-1  ]: SQL status: OK in 4.06 seconds
[0m20:25:27.528984 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_users_cities_join (execute): 2023-03-30 20:25:23.393951 => 2023-03-30 20:25:27.528914
[0m20:25:27.529348 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: ROLLBACK
[0m20:25:27.529574 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:25:27.529782 [debug] [Thread-1  ]: On model.dbx_cities.silver_users_cities_join: Close
[0m20:25:27.683968 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba34f9e6-b695-4d90-836f-a3253696df2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f032cd0>]}
[0m20:25:27.685958 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_users_cities_join ............. [[32mOK[0m in 4.29s]
[0m20:25:27.690482 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_users_cities_join
[0m20:25:27.693384 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:25:27.693993 [debug] [MainThread]: On master: ROLLBACK
[0m20:25:27.694423 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:25:27.887765 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:25:27.889286 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:25:27.890146 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:25:27.891021 [debug] [MainThread]: On master: ROLLBACK
[0m20:25:27.891648 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:25:27.891963 [debug] [MainThread]: On master: Close
[0m20:25:28.060561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:25:28.061728 [debug] [MainThread]: Connection 'model.dbx_cities.silver_users_cities_join' was properly closed.
[0m20:25:28.063817 [info ] [MainThread]: 
[0m20:25:28.064600 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.35 seconds (7.35s).
[0m20:25:28.065545 [debug] [MainThread]: Command end result
[0m20:25:28.076955 [info ] [MainThread]: 
[0m20:25:28.077565 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:25:28.078004 [info ] [MainThread]: 
[0m20:25:28.078445 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:25:28.078969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f05e0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e681f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e681dc0>]}
[0m20:25:28.079370 [debug] [MainThread]: Flushing usage events
